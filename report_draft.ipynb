{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "When multiple drugs are used in treatment of a patient, there is a possibility drugs interact, potentially causing harmful effects.\n",
    "To reduce drug development time and uncover negative interactions before human trials, clinicians need a way to predict these drug interactions. This problem is combinatorially difficult due to the massive number of drugs, variations in drug features such as molecular structure, and the range of potential interactions. By using neural networks, clinicians could uncover patterns in drug interaction, enabling better understanding of drug functionality on the body. This understanding could not only discover negative drug interactions, but also secondary use-cases of drugs for treatment.\n",
    "\n",
    "\n",
    "Traditional model architectures, such as proposed by $Rohani^{1}$ utilize drug-similarity matrices of features such as molecular substructure to predict interactions. These matrices are sub-selected for the most informative and fused into a singular matrix as input for a neural network. These approaches omit critical information about the kind of interactions between drugs, and simply discover any interaction. For clinicians to properly understand drug functionality and safety of use, they must be able to predict the severity and type of drug interactions.\n",
    "\n",
    "\n",
    "Graph Neural Networks have recently been employed to encode and explore various drug interaction types. By encoding drugs as nodes, and interaction types as edges, models can better understand how drugs interact with one another. However, generating graph representations of drug interactions is challenging, particularly in how to combine various features from various sources and how to generate embeddings. $Lin et al^{2}$, generates a graph structure from various features, but only predicts the presence of an interaction from node locality - ignoring the semantic representation of features.\n",
    "\n",
    "\n",
    "$Al-Rabeah^{3}$ improves upon both the traditional and GNN approaches by utilizing graph networks of various features to generate embeddings for feeding into a neural network for classification. The aim is to integrate both the similarity matrices of previous approaches with GNN embeddings of multiple featuresets, in order to represent a heterogeneous network for predicting the types of drug-drug interactions.\n",
    "\n",
    "\n",
    "The work utilizes four feature matrices (Chemical structure, Target, Enzyme, and Pathway) as attributes for nodes representing drugs via similarity matrices, with edges being the drug-drug interaction. Each graph is then used to generate an embedding matrix with a vector for each drug and interaction type. This is done via a random walk from the drug node utilizing edges of the given interaction type to form a node sequence, which is then used to learn embeddings. These four embedding matrices are sliced and cross-multiplied within each other to represent drug-drug interactions. These vectors are then fed into neural networks for each matrix, using a softmax to generate probability vectors for all event types. These vectors are fed through ReLU activation to produce a final prediction vector.\n",
    "\n",
    "\n",
    "The work tested using all subset of the feature matrices, with the best performance from usage all four matrices gathered. The results, achieving 0.9206, 0.9992, 0.9717, 0.8579, and 0.8259 Accuracy, AUC, AUPR, F1, and recall scores respectively, outperformed many recent DDI works. This showed significant improvement over traditional Neural Network approaches, such as CNN-DDI by $Zhang^{4}$. While other GNN approaches, such as the novel KGNN, achieved better performance in some metrics such as F1 score - their analysis was over individual datasets and neglected several features. From this, the GNN-DDI architecture showed exceptional performance as it was faced with multiple-datasets, indicating strong usage across various features.\n",
    "\n",
    "\n",
    "The work of $Al-Rabeah^{3}$ showed the strength of combining various feature sets for generating neural network inputs. Likewise, the paper exemplified the utility in combining multiple components of past approaches (GNN and fusion of multiple similarity matrix results) to produce a robust architecture performant over  features from multiple datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope of Reproducibility\n",
    "\n",
    "\n",
    "We first re-implemented the entire code repository of the paper using PyTorch. The original code provided was extremely hard to understand and modify, using obscure, lengthy matrix transformations to perform general operations. By re-writing in PyTorch, we found it easier to implement various modifications/ablations within the original codebase.\n",
    "\n",
    "\n",
    "With the Pytorch code, we tested the below hypotheses and ablations:\n",
    "\n",
    "\n",
    "1. Hyperparameter Tuning.\n",
    "   By varying the below hyperparameters, we attempt to achieve a higher performance from the model. The paper performs minimal hyperparameter tuning, so we anticipate minor tuning to significantly improve performance. In addition, the original paper uses the Adam optimizer which is relatively fast to converge on local minima. By optimizing parameters, we could achieve a better minimum for training our models.\n",
    "   - Layer size\n",
    "       - Varying the sizes of intermediary layers can constrain the model's performance to fit to training data. By reducing intermediary layer size, we can have a looser bias to the training set. This could be used to lower dropout rate, enabling faster training without sacrificing model generalization.\n",
    "   - Number of dense layers\n",
    "       - The dataset utilized by the paper is unbalanced leading to underfitting. We anticipate increasing the number of dense layers (2) within the neural networks will result in a more biased model - which could better fit the intricacies of less prevalent drug-drug interactions.\n",
    "   - Dropout rate\n",
    "       - The paper uses a dropout rate of 0.3 to inject random noise and reduce bias. By modifying dropout rate, alongside changes to layer size, we anticipate the same model generalization with improved fitting to training data.\n",
    "2. Removing Dropout, and replacing with optimizer weight decay.\n",
    "   Dropout introduces randomness into the network, while weight decay promotes a balanced model by regulating heavy weights. We anticipate weight decay to more effectively account for relations in similarity matrices, particularly for interactions less present in the training set, leading to a higher CV score.\n",
    "3. Replacing the Adam optimizer with SGD.\n",
    "   The paper did not tune the parameters of the Adam optimizer, which likely led to convergence on a suboptimal minima. SGD is less likely to converge on these minima, and should result in better generalizability than untuned Adam.\n",
    "4. Removal of dropout layers.\n",
    "   The proposed GNN relies on two hidden-layer 0.3 rate dropout layers to prevent overfitting. We will test the removal of each hidden layer, as well as both layers. By removing these layers, we can evaluate how tightly model overfits to the similarity matrices.\n",
    "5. Removal of early stopping\n",
    "   The code utilizes early stopping to prevent model overfitting if performance doesnâ€™t improve in 10 epochs. We will remove this limitation, and attempt to overfit our network while retaining dropout. This will help evaluate how injected noise impacts performance.\n",
    "6. Combining of 4 feedforward networks into 1\n",
    "   The paper uses 4 feedforward networks to evaluate each property type independently and aggregates the results, however this does not capture any interactions between property types. We will test concatenating the feature vectors from each property type and using a single feedforward network with this feature vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "\n",
    "The paper utilizes four 2-D feature matrices of 572 drugs, derived from different databases:\n",
    "* Chemical Structure\n",
    "   - SMILE strings, ASCII representations, of drug molecule structures. Commonly used to encode molecular structure.\n",
    "* Target\n",
    "   - The molecule in the body which drug is intended to impact.\n",
    "* Enzyme\n",
    "   - The enzyme responsible for processing a drug within the body. Enzymes can degrade molecules into derivatives.\n",
    "* Pathway\n",
    "   - The specific pathway through which drugs are absorbed, distributed, metabolized, and excreted from the body.\n",
    "\n",
    "\n",
    "All feature matrices were derived from the DrugBank database, and sourced from $Deng et al^{5}$ as an _eventdb_. The Pathway feature matrix also includes data from the KEGG database. Each column in the matrices represents an individual drug, while the rows are 1-hot encodings of the presence of drug properties. As a result, each matrix varies in the length of the second axis.\n",
    "\n",
    "\n",
    "To achieve a representation of drug interactions, each of these feature matrices are transformed into similarity matrices via Jacquard similarity function. This results in uniform 572x572 similarity feature matrices. These similarity matrices were provided, however we also utilized our own code to derive these matrices. The similarity matrices become the node attributes for our various graphs.\n",
    "\n",
    "\n",
    "In addition, the paper utilizes a 4-D drug interaction matrix, describing 65 types of interactions between the 572 drugs. These 65 types are the types of DDI events extracted by $Deng et al^{5}$. This matrix is used as the nodes and edges of our drug graphs. The matrix axes correspond to two drugs, their mechanism for interaction, and if the interaction is increased or decreased. From this, the paper can derive two submatrices: the presence of drug interactions and interaction types. The distribution of the data is extremely uneven across event types, resulting in underfitting to the less represented event. This fact informed the basis of many of our experiments - as we try to appropriately fit the model for these events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Connfiguration\n",
    "\n",
    "This paper has several steps:\n",
    "* Download event_db and extract raw drug attributes and interaction data.\n",
    "* Train GNN models and generate graph embeddings\n",
    "* Train NN\n",
    "\n",
    "For ease of running, we will enable feature_flags, selecting to either fully run each step OR download from precomputed sources. Any step can be toggled on/off in the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook feature flags.\n",
    "\n",
    "# == Data Flags ==\n",
    "\n",
    "# Set the below main feature flags for default settings.\n",
    "# If neither selected, default is to use precomputed values.\n",
    "_ALWAYS_GENERATE_DATA = True\n",
    "_ALWAYS_SAVE_DATA = True\n",
    "\n",
    "\n",
    "# Default Feature Flags\n",
    "\n",
    "# == Data Flags == \n",
    "\n",
    "_DOWNLOAD_EVENTDB_AND_CONSTRUCT_JACCARD_MATRICES = False\n",
    "# Download event db and manually construct jaccard similarity matrices\n",
    "\n",
    "_SAVE_CONSTRUCTED_JACCARD_MATRICES = False\n",
    "# Save constructed jaccard matrices for quick reloading.\n",
    "# Only applicable if _DOWNLOAD_EVENTDB_AND_CONSTRUCT_JACCARD_MATRICES is true.\n",
    "\n",
    "_DOWNLOAD_EVENTDB_AND_CONSTRUCT_INTERACTION_MATRICES = False\n",
    "# Download event db and manually construct drug interaction matrices\n",
    "\n",
    "_SAVE_CONSTRUCTED_INTERACTION_MATRICES = False\n",
    "# Save constructed interaction matrices for quick reloading.\n",
    "# Only applicable if _DOWNLOAD_EVENTDB_AND_CONSTRUCT_INTERACTION_MATRICES is true.\n",
    "\n",
    "# == Training Flags == \n",
    "\n",
    "_TRAIN_GNN_MODELS_AND_GENERATE_EMBEDDINGS = False\n",
    "_SAVE_CONSTRUCTED_GNN_MODELS = False\n",
    "\n",
    "if _ALWAYS_GENERATE_DATA:\n",
    "    _DOWNLOAD_EVENTDB_AND_CONSTRUCT_JACCARD_MATRICES = True\n",
    "    _DOWNLOAD_EVENTDB_AND_CONSTRUCT_INTERACTION_MATRICES = True\n",
    "    _TRAIN_GNN_MODELS_AND_GENERATE_EMBEDDINGS = True\n",
    "\n",
    "if _ALWAYS_SAVE_DATA:\n",
    "    _SAVE_CONSTRUCTED_JACCARD_MATRICES = True\n",
    "    _SAVE_CONSTRUCTED_INTERACTION_MATRICES = True\n",
    "    _SAVE_CONSTRUCTED_GNN_MODELS = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Raw Feature Data\n",
    "\n",
    "Download and extract raw feature data from event_db (from Deng et al) and convert into the required Jacard similarity matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file to /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/event_db.txt\n",
      "Saved target Jaccard similarity matrix to /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/target_jaccard.parquet\n",
      "Saved pathway Jaccard similarity matrix to /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/pathway_jaccard.parquet\n",
      "Saved smile Jaccard similarity matrix to /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/smile_jaccard.parquet\n",
      "Saved enzyme Jaccard similarity matrix to /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/enzyme_jaccard.parquet\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Mapping\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Constants\n",
    "_CHECKMARKS_FOLDER = Path.cwd() / \"checkmarks\"\n",
    "_CHECKMARKS_FOLDER.mkdir(exist_ok=True)\n",
    "_EVENT_DB_URL: str = 'https://raw.githubusercontent.com/NealRyan/Drug-drug-interaction-modeling/main/Data/Raw%20Event%20DB/event.db'\n",
    "_EVENT_DB_FILE_PATH: Path = _CHECKMARKS_FOLDER / \"event_db.txt\"\n",
    "\n",
    "_FEATURE_TYPES = {\"target\", \"enzyme\", \"pathway\", \"smile\"}\n",
    "_FEATURE_JAC_MATRICES = {}\n",
    "\n",
    "def construct_local_matrix_path(feature_type: str) -> Path:\n",
    "    return _CHECKMARKS_FOLDER / f\"{feature_type}_jaccard.parquet\"\n",
    "\n",
    "def download_and_generate_matrices():\n",
    "    \"\"\"Manually download and generate Jaccard similarity matrices.\"\"\"\n",
    "    # Helper functions\n",
    "    def download_file(url: str, file_path: Path) -> None:\n",
    "        response = requests.get(url)\n",
    "        with file_path.open('wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f'Downloaded file to {file_path}')\n",
    "\n",
    "    # Download event and extract raw features\n",
    "    download_file(_EVENT_DB_URL, _EVENT_DB_FILE_PATH)\n",
    "\n",
    "    conn = sqlite3.connect(_EVENT_DB_FILE_PATH)\n",
    "\n",
    "    df_raw_drug = pd.read_sql('select * from drug;', conn)\n",
    "\n",
    "    # close the DB connection\n",
    "    conn.close()\n",
    "\n",
    "    # Transform downloaded raw features into jaccard similarity matrices.\n",
    "    num_drugs = len(df_raw_drug.index)\n",
    "    for feature in _FEATURE_TYPES:\n",
    "        raw_feature_series = df_raw_drug[feature]\n",
    "\n",
    "        # Generate a global list of all elements\n",
    "        # and a mapping of drug idx to its relevant elements\n",
    "        element_list = []\n",
    "        drug_element_mapping = {}\n",
    "        for drug_idx, cell in enumerate(raw_feature_series):\n",
    "            split_element_list = cell.split('|')\n",
    "            drug_element_mapping[drug_idx] = list(range(len(element_list), len(element_list) + len(split_element_list)))\n",
    "            element_list.extend(split_element_list)\n",
    "\n",
    "        # For each drug, set its relevant elements values to False\n",
    "        jaccard = np.full((num_drugs, len(element_list)), False)\n",
    "        for drug_idx, element_idxs in drug_element_mapping.items():\n",
    "            jaccard[drug_idx, element_idxs] = True\n",
    "        \n",
    "        # Perform jaccard similarity\n",
    "        jac_sim_df = pd.DataFrame(\n",
    "            1 - pairwise_distances(jaccard, metric='jaccard')\n",
    "        )\n",
    "\n",
    "        # Save dataclass for later use.\n",
    "        _FEATURE_JAC_MATRICES[feature] = jac_sim_df\n",
    "\n",
    "        if _SAVE_CONSTRUCTED_JACCARD_MATRICES:\n",
    "            output_path = construct_local_matrix_path(feature)\n",
    "            jac_sim_df.to_parquet(output_path)\n",
    "            print(f\"Saved {feature} Jaccard similarity matrix to {output_path}\")\n",
    "\n",
    "def load_precomputed_similarity_matrices() -> None:\n",
    "    \"\"\"Load precomputed Jacard similarity matrices.\"\"\"\n",
    "    print(\"Using precomputed similarity matrices.\")\n",
    "    for feature in _FEATURE_TYPES:\n",
    "        _FEATURE_JAC_MATRICES[feature] = pd.read_parquet(construct_local_matrix_path(feature))\n",
    "\n",
    "if _DOWNLOAD_EVENTDB_AND_CONSTRUCT_JACCARD_MATRICES:\n",
    "    download_and_generate_matrices()\n",
    "else:\n",
    "    load_precomputed_similarity_matrices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract drug event type interactions from event_db (from Deng et al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dfs = {\n",
    "    'pos': None,\n",
    "    'neg': None,\n",
    "    'raw': None,\n",
    "}\n",
    "\n",
    "def construct_local_interaction_path(list_name: str) -> Path:\n",
    "    return _CHECKMARKS_FOLDER / f\"{list_name}_interaction.parquet\"\n",
    "\n",
    "\n",
    "def generate_drug_interaction_matrices() -> None:\n",
    "    conn = sqlite3.connect(_EVENT_DB_FILE_PATH)\n",
    "    df_raw_extraction = pd.read_sql('select * from extraction;', conn)\n",
    "    df_raw_drug = pd.read_sql('select * from drug;', conn)\n",
    "    conn.close()\n",
    "\n",
    "    df_raw_drug = df_raw_drug.set_index('name') \n",
    "\n",
    "    df_raw_extraction['event'] = df_raw_extraction['mechanism'] + ' ' + df_raw_extraction['action']\n",
    "\n",
    "    df_raw_events = df_raw_extraction['event'].value_counts().to_frame()\n",
    "    df_raw_events['event_index'] = np.arange(df_raw_events.shape[0])\n",
    "\n",
    "    df_raw_full_pos = df_raw_extraction.join(df_raw_events, on='event')[['drugA', 'drugB', 'event_index']]\n",
    "    df_raw_full_pos = df_raw_full_pos.join(df_raw_drug['index'], on='drugA')\n",
    "    df_raw_full_pos = df_raw_full_pos.join(df_raw_drug['index'], on='drugB', rsuffix='_drugB')\n",
    "    df_raw_full_pos = df_raw_full_pos[['event_index', 'index', 'index_drugB']]\n",
    "\n",
    "    df_raw_events = df_raw_events.reset_index()[['event', 'count']]\n",
    "\n",
    "    used_pairs = np.identity(len(df_raw_drug.index))\n",
    "    # could also sort ints to ensure consistent ordering instead of making matrix symmetrical\n",
    "    for _, row in df_raw_full_pos.iterrows():\n",
    "        used_pairs[row['index'], row['index_drugB']] = 1\n",
    "        used_pairs[row['index_drugB'], row['index']] = 1\n",
    "\n",
    "    df_raw_all_neg = np.zeros((len(df_raw_full_pos.index), 2))\n",
    "\n",
    "    count = 0\n",
    "    while count < len(df_raw_all_neg):\n",
    "        # could also sort ints to ensure consistent ordering instead of making matrix symmetrical\n",
    "        rand_pair = np.random.randint(len(df_raw_drug.index), size=2)\n",
    "        while used_pairs[rand_pair[0], rand_pair[1]] == 1 or used_pairs[rand_pair[1], rand_pair[0]] == 1:\n",
    "            rand_pair = np.random.randint(len(df_raw_drug.index), size=2)\n",
    "\n",
    "        used_pairs[rand_pair[0], rand_pair[1]] = 1\n",
    "        used_pairs[rand_pair[1], rand_pair[0]] = 1\n",
    "        df_raw_all_neg[count] = rand_pair\n",
    "        count += 1\n",
    "\n",
    "    df_raw_all_neg = pd.DataFrame(df_raw_all_neg)\n",
    "\n",
    "    event_dfs['pos'] = df_raw_full_pos\n",
    "    event_dfs['neg'] = df_raw_all_neg\n",
    "    event_dfs['raw'] = df_raw_events\n",
    "\n",
    "    if _SAVE_CONSTRUCTED_INTERACTION_MATRICES:\n",
    "        for key, df in event_dfs.items():\n",
    "            df.to_parquet(construct_local_interaction_path(key))\n",
    "        \n",
    "\n",
    "def load_precomputed_interaction_matrices() -> None:\n",
    "    for key in event_dfs.keys():\n",
    "        event_dfs[key] = pd.read_parquet(construct_local_interaction_path(key))\n",
    "\n",
    "if _DOWNLOAD_EVENTDB_AND_CONSTRUCT_INTERACTION_MATRICES:\n",
    "    generate_drug_interaction_matrices()\n",
    "else:\n",
    "    load_precomputed_interaction_matrices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "For training, there are two portions:\n",
    "* Generating graph embeddings from the generated event_db data above\n",
    "\n",
    "#### Generating Graph Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_length = 10 #Default 10\n",
    "walks_per_node = 20 #Default 20\n",
    "window_size = 5 #Default 5\n",
    "negative_samples = 5 #Default 5\n",
    "learning_rate = 0.01 #Default 0.01\n",
    "epochs = 1 #Default 1\n",
    "batch_size=64 #Default 64, lower this if RAM spikes are crashing Colab\n",
    "num_workers=16 #Default 16\n",
    "patience_num = 5 #no default (because we default to 1 epoch)\n",
    "save_path = '/content'\n",
    "edge_dim = 10 #Degault 10\n",
    "att_dim = 20 #Default 20\n",
    "negative_samples = 5 #Default 5\n",
    "neighbor_samples = 20 #Default 10\n",
    "eval_type = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def walk(args):\n",
    "    walk_length, start, schema = args\n",
    "    # Simulate a random walk starting from start node.\n",
    "    rand = random.Random()\n",
    "\n",
    "    if schema:\n",
    "        schema_items = schema.split('-')\n",
    "        assert schema_items[0] == schema_items[-1]\n",
    "\n",
    "    walk = [start]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        candidates = []\n",
    "        for node in G[cur]:\n",
    "            if schema == '' or node_type[node] == schema_items[len(walk) % (len(schema_items) - 1)]:\n",
    "                candidates.append(node)\n",
    "        if candidates:\n",
    "            walk.append(rand.choice(candidates))\n",
    "        else:\n",
    "            break\n",
    "    return [str(node) for node in walk]\n",
    "\n",
    "def initializer(init_G, init_node_type):\n",
    "    global G\n",
    "    G = init_G\n",
    "    global node_type\n",
    "    node_type = init_node_type\n",
    "\n",
    "class RWGraph():\n",
    "    def __init__(self, nx_G, node_type_arr=None, num_workers=16):\n",
    "        self.G = nx_G\n",
    "        self.node_type = node_type_arr\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def node_list(self, nodes, num_walks):\n",
    "        for loop in range(num_walks):\n",
    "            for node in nodes:\n",
    "                yield node\n",
    "\n",
    "    def simulate_walks(self, num_walks, walk_length, schema=None):\n",
    "        all_walks = []\n",
    "        nodes = list(self.G.keys())\n",
    "        random.shuffle(nodes)\n",
    "\n",
    "        if schema is None:\n",
    "            with multiprocessing.Pool(self.num_workers, initializer=initializer, initargs=(self.G, self.node_type)) as pool:\n",
    "                all_walks = list(pool.imap(walk, ((walk_length, node, '') for node in tqdm(self.node_list(nodes, num_walks))), chunksize=256))\n",
    "        else:\n",
    "            schema_list = schema.split(',')\n",
    "            for schema_iter in schema_list:\n",
    "                with multiprocessing.Pool(self.num_workers, initializer=initializer, initargs=(self.G, self.node_type)) as pool:\n",
    "                    walks = list(pool.imap(walk, ((walk_length, node, schema_iter) for node in tqdm(self.node_list(nodes, num_walks)) if schema_iter.split('-')[0] == self.node_type[node]), chunksize=512))\n",
    "                all_walks.extend(walks)\n",
    "\n",
    "        return all_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from six import iteritems\n",
    "from sklearn.metrics import (auc, f1_score, precision_recall_curve,\n",
    "                             roc_auc_score)\n",
    "\n",
    "@dataclass\n",
    "class Vocab():\n",
    "\n",
    "    count: int\n",
    "    index: int\n",
    "\n",
    "#Batches for training the GNN\n",
    "def get_batches(pairs, neighbors, batch_size):\n",
    "    n_batches = (len(pairs) + (batch_size - 1)) // batch_size\n",
    "\n",
    "    for idx in range(n_batches):\n",
    "        x, y, t, neigh = [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            index = idx * batch_size + i\n",
    "            if index >= len(pairs):\n",
    "                break\n",
    "            x.append(pairs[index][0])\n",
    "            y.append(pairs[index][1])\n",
    "            t.append(pairs[index][2])\n",
    "            neigh.append(neighbors[pairs[index][0]])\n",
    "        yield torch.tensor(x), torch.tensor(y), torch.tensor(t), torch.tensor(neigh)\n",
    "\n",
    "def get_G_from_edges(edges):\n",
    "    edge_dict = defaultdict(set)\n",
    "    for edge in edges:\n",
    "        u, v = str(edge[0]), str(edge[1])\n",
    "        edge_dict[u].add(v)\n",
    "        edge_dict[v].add(u)\n",
    "    return edge_dict\n",
    "\n",
    "def load_training_data(f_name):\n",
    "    print('Loading data from:', f_name)\n",
    "    edge_data_by_type = dict()\n",
    "    all_nodes = list()\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(' ')\n",
    "            if words[0] not in edge_data_by_type:\n",
    "                edge_data_by_type[words[0]] = list()\n",
    "            x, y = words[1], words[2]\n",
    "            edge_data_by_type[words[0]].append((x, y))\n",
    "            all_nodes.append(x)\n",
    "            all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    print('Total training nodes: ' + str(len(all_nodes)))\n",
    "    return edge_data_by_type\n",
    "\n",
    "\n",
    "def load_testing_data(f_name):\n",
    "    print('Loading data from:', f_name)\n",
    "    true_edge_data_by_type = dict()\n",
    "    false_edge_data_by_type = dict()\n",
    "    all_nodes = list()\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(' ')\n",
    "            x, y = words[1], words[2]\n",
    "            if int(words[3]) == 1:\n",
    "                if words[0] not in true_edge_data_by_type:\n",
    "                    true_edge_data_by_type[words[0]] = list()\n",
    "                true_edge_data_by_type[words[0]].append((x, y))\n",
    "            else:\n",
    "                if words[0] not in false_edge_data_by_type:\n",
    "                    false_edge_data_by_type[words[0]] = list()\n",
    "                false_edge_data_by_type[words[0]].append((x, y))\n",
    "            all_nodes.append(x)\n",
    "            all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    return true_edge_data_by_type, false_edge_data_by_type\n",
    "\n",
    "def load_node_type(f_name):\n",
    "    print('Loading node type from:', f_name)\n",
    "    node_type = {}\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            node_type[items[0]] = items[1]\n",
    "    return node_type\n",
    "\n",
    "def load_feature_data(f_name):\n",
    "    feature_dic = {}\n",
    "    with open(f_name, 'r') as f:\n",
    "        first = True\n",
    "        for line in f:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            items = line.strip().split()\n",
    "            feature_dic[items[0]] = items[1:]\n",
    "    return feature_dic\n",
    "\n",
    "def generate_walks(network_data, num_walks, walk_length, schema, file_name, num_workers):\n",
    "    if schema is not None:\n",
    "        node_type = load_node_type(file_name + '/node_type.txt')\n",
    "    else:\n",
    "        node_type = None\n",
    "\n",
    "    all_walks = []\n",
    "    for layer_id, layer_name in enumerate(network_data):\n",
    "        tmp_data = network_data[layer_name]\n",
    "        # start to do the random walk on a layer\n",
    "\n",
    "        layer_walker = RWGraph(get_G_from_edges(tmp_data), node_type, num_workers)\n",
    "        print('Generating random walks for event_num', layer_id)\n",
    "        layer_walks = layer_walker.simulate_walks(num_walks, walk_length, schema=schema)\n",
    "\n",
    "        all_walks.append(layer_walks)\n",
    "\n",
    "    print('Finish generating the walks')\n",
    "\n",
    "    return all_walks\n",
    "\n",
    "def generate_pairs(all_walks, vocab, window_size, num_workers):\n",
    "    pairs = []\n",
    "    skip_window = window_size // 2\n",
    "    for layer_id, walks in enumerate(all_walks):\n",
    "        print('Generating training pairs for event_num', layer_id)\n",
    "        for walk in tqdm(walks):\n",
    "            for i in range(len(walk)):\n",
    "                for j in range(1, skip_window + 1):\n",
    "                    if i - j >= 0:\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i - j]].index, layer_id))\n",
    "                    if i + j < len(walk):\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i + j]].index, layer_id))\n",
    "    return pairs\n",
    "\n",
    "def generate_vocab(all_walks):\n",
    "    index2word = []\n",
    "    raw_vocab = defaultdict(int)\n",
    "\n",
    "    for layer_id, walks in enumerate(all_walks):\n",
    "        print('Counting vocab for event_num', layer_id)\n",
    "        for walk in tqdm(walks):\n",
    "            for word in walk:\n",
    "                raw_vocab[word] += 1\n",
    "\n",
    "    vocab = {}\n",
    "    for word, v in iteritems(raw_vocab):\n",
    "        vocab[word] = Vocab(count=v, index=len(index2word))\n",
    "        index2word.append(word)\n",
    "\n",
    "    index2word.sort(key=lambda word: vocab[word].count, reverse=True)\n",
    "    for i, word in enumerate(index2word):\n",
    "        vocab[word].index = i\n",
    "\n",
    "    return vocab, index2word\n",
    "\n",
    "def load_walks(walk_file):\n",
    "    print('Loading walks')\n",
    "    all_walks = []\n",
    "    with open(walk_file, 'r') as f:\n",
    "        for line in f:\n",
    "            content = line.strip().split()\n",
    "            layer_id = int(content[0])\n",
    "            if layer_id >= len(all_walks):\n",
    "                all_walks.append([])\n",
    "            all_walks[layer_id].append(content[1:])\n",
    "    return all_walks\n",
    "\n",
    "def save_walks(walk_file, all_walks):\n",
    "    with open(walk_file, 'w') as f:\n",
    "        for layer_id, walks in enumerate(all_walks):\n",
    "            print('Saving walks for event_num', layer_id)\n",
    "            for walk in tqdm(walks):\n",
    "                f.write(' '.join([str(layer_id)] + [str(x) for x in walk]) + '\\n')\n",
    "\n",
    "def generate(network_data, num_walks, walk_length, schema, file_name, window_size, num_workers, walk_file):\n",
    "    if walk_file is not None:\n",
    "        all_walks = load_walks(walk_file)\n",
    "    else:\n",
    "        all_walks = generate_walks(network_data, num_walks, walk_length, schema, file_name, num_workers)\n",
    "        save_walks(file_name + '/walks.txt', all_walks)\n",
    "    vocab, index2word = generate_vocab(all_walks)\n",
    "    train_pairs = generate_pairs(all_walks, vocab, window_size, num_workers)\n",
    "\n",
    "    return vocab, index2word, train_pairs\n",
    "\n",
    "def generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples):\n",
    "    edge_type_count = len(edge_types)\n",
    "    neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]\n",
    "    for r in range(edge_type_count):\n",
    "        print('Generating neighbors for event_num', r)\n",
    "        g = network_data[edge_types[r]]\n",
    "        for (x, y) in tqdm(g):\n",
    "            ix = vocab[x].index\n",
    "            iy = vocab[y].index\n",
    "            neighbors[ix][r].append(iy)\n",
    "            neighbors[iy][r].append(ix)\n",
    "        for i in range(num_nodes):\n",
    "            if len(neighbors[i][r]) == 0:\n",
    "                neighbors[i][r] = [i] * neighbor_samples\n",
    "            elif len(neighbors[i][r]) < neighbor_samples:\n",
    "                neighbors[i][r].extend(list(np.random.choice(neighbors[i][r], size=neighbor_samples-len(neighbors[i][r]))))\n",
    "            elif len(neighbors[i][r]) > neighbor_samples:\n",
    "                neighbors[i][r] = list(np.random.choice(neighbors[i][r], size=neighbor_samples))\n",
    "    return neighbors\n",
    "\n",
    "def get_score(local_model, node1, node2):\n",
    "    try:\n",
    "        vector1 = local_model[node1]\n",
    "        vector2 = local_model[node2]\n",
    "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "def evaluate(model, true_edges, false_edges):\n",
    "    true_list = list()\n",
    "    prediction_list = list()\n",
    "    true_num = 0\n",
    "    for edge in true_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        if tmp_score is not None:\n",
    "            true_list.append(1)\n",
    "            prediction_list.append(tmp_score)\n",
    "            true_num += 1\n",
    "\n",
    "    for edge in false_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        if tmp_score is not None:\n",
    "            true_list.append(0)\n",
    "            prediction_list.append(tmp_score)\n",
    "\n",
    "    sorted_pred = prediction_list[:]\n",
    "    sorted_pred.sort()\n",
    "    threshold = sorted_pred[-true_num]\n",
    "\n",
    "    y_pred = np.zeros(len(prediction_list), dtype=np.int32)\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] >= threshold:\n",
    "            y_pred[i] = 1\n",
    "\n",
    "    y_true = np.array(true_list)\n",
    "    y_scores = np.array(prediction_list)\n",
    "    ps, rs, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return roc_auc_score(y_true, y_scores), f1_score(y_true, y_pred), auc(rs, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class GATNEModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a, features\n",
    "    ):\n",
    "        super(GATNEModel, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_u_size = embedding_u_size\n",
    "        self.edge_type_count = edge_type_count\n",
    "        self.dim_a = dim_a\n",
    "\n",
    "        self.features = None\n",
    "        if features is not None:\n",
    "            self.features = features\n",
    "            feature_dim = self.features.shape[-1]\n",
    "            self.embed_trans = Parameter(torch.FloatTensor(feature_dim, embedding_size))\n",
    "            self.u_embed_trans = Parameter(torch.FloatTensor(edge_type_count, feature_dim, embedding_u_size))\n",
    "        else:\n",
    "            self.node_embeddings = Parameter(torch.FloatTensor(num_nodes, embedding_size))\n",
    "            self.node_type_embeddings = Parameter(\n",
    "                torch.FloatTensor(num_nodes, edge_type_count, embedding_u_size)\n",
    "            )\n",
    "        self.trans_weights = Parameter(\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, embedding_size)\n",
    "        )\n",
    "        self.trans_weights_s1 = Parameter(\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, dim_a)\n",
    "        )\n",
    "        self.trans_weights_s2 = Parameter(torch.FloatTensor(edge_type_count, dim_a, 1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.features is not None:\n",
    "            self.embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "            self.u_embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        else:\n",
    "            self.node_embeddings.data.uniform_(-1.0, 1.0)\n",
    "            self.node_type_embeddings.data.uniform_(-1.0, 1.0)\n",
    "        self.trans_weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s1.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s2.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "\n",
    "    def forward(self, train_inputs, train_types, node_neigh):\n",
    "        if self.features is None:\n",
    "            node_embed = self.node_embeddings[train_inputs]\n",
    "            node_embed_neighbors = self.node_type_embeddings[node_neigh]\n",
    "        else:\n",
    "            node_embed = torch.mm(self.features[train_inputs], self.embed_trans)\n",
    "            node_embed_neighbors = torch.einsum('bijk,akm->bijam', self.features[node_neigh], self.u_embed_trans)\n",
    "        node_embed_tmp = torch.diagonal(node_embed_neighbors, dim1=1, dim2=3).permute(0, 3, 1, 2)\n",
    "        node_type_embed = torch.sum(node_embed_tmp, dim=2)\n",
    "\n",
    "        trans_w = self.trans_weights[train_types]\n",
    "        trans_w_s1 = self.trans_weights_s1[train_types]\n",
    "        trans_w_s2 = self.trans_weights_s2[train_types]\n",
    "\n",
    "        attention = F.softmax(\n",
    "            torch.matmul(\n",
    "                torch.tanh(torch.matmul(node_type_embed, trans_w_s1)), trans_w_s2\n",
    "            ).squeeze(2),\n",
    "            dim=1,\n",
    "        ).unsqueeze(1)\n",
    "        node_type_embed = torch.matmul(attention, node_type_embed)\n",
    "        node_embed = node_embed + torch.matmul(node_type_embed, trans_w).squeeze(1)\n",
    "\n",
    "        last_node_embed = F.normalize(node_embed, dim=1)\n",
    "\n",
    "        return last_node_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSLoss(nn.Module):\n",
    "    def __init__(self, num_nodes, num_sampled, embedding_size):\n",
    "        super(NSLoss, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_sampled = num_sampled\n",
    "        self.embedding_size = embedding_size\n",
    "        self.weights = Parameter(torch.FloatTensor(num_nodes, embedding_size))\n",
    "        self.sample_weights = F.normalize(\n",
    "            torch.Tensor(\n",
    "                [\n",
    "                    (math.log(k + 2) - math.log(k + 1)) / math.log(num_nodes + 1)\n",
    "                    for k in range(num_nodes)\n",
    "                ]\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "\n",
    "    def forward(self, input, embs, label):\n",
    "        n = input.shape[0]\n",
    "        log_target = torch.log(\n",
    "            torch.sigmoid(torch.sum(torch.mul(embs, self.weights[label]), 1))\n",
    "        )\n",
    "        negs = torch.multinomial(\n",
    "            self.sample_weights, self.num_sampled * n, replacement=True\n",
    "        ).view(n, self.num_sampled)\n",
    "        noise = torch.neg(self.weights[negs])\n",
    "        sum_log_sampled = torch.sum(\n",
    "            torch.log(torch.sigmoid(torch.bmm(noise, embs.unsqueeze(2)))), 1\n",
    "        ).squeeze()\n",
    "\n",
    "        loss = log_target + sum_log_sampled\n",
    "        return -loss.sum() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network_data, feature_dic, feature_name):\n",
    "    vocab, index2word, train_pairs = generate(network_data, walks_per_node, walk_length, schema=None, file_name=save_path, window_size=window_size, num_workers=num_workers, walk_file=None)\n",
    "\n",
    "    edge_types = list(network_data.keys())\n",
    "\n",
    "    num_nodes = len(index2word)\n",
    "    edge_type_count = len(edge_types)\n",
    "    #epochs = epochs\n",
    "    #batch_size = args.batch_size\n",
    "    embedding_size = embedding_dim\n",
    "    embedding_u_size = edge_dim\n",
    "    u_num = edge_type_count\n",
    "    num_sampled = negative_samples\n",
    "    dim_a = att_dim\n",
    "    att_head = 1\n",
    "    #neighbor_samples = neighbor_samples\n",
    "    print(u_num)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    neighbors = generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples)\n",
    "\n",
    "    features = None\n",
    "    if feature_dic is not None:\n",
    "        feature_dim = len(list(feature_dic.values())[0])\n",
    "        print('feature dimension: ' + str(feature_dim))\n",
    "        features = np.zeros((num_nodes, feature_dim), dtype=np.float32)\n",
    "        for key, value in feature_dic.items():\n",
    "            if key in vocab:\n",
    "                features[vocab[key].index, :] = np.array(value)\n",
    "        features = torch.FloatTensor(features).to(device)\n",
    "\n",
    "    model = GATNEModel(\n",
    "        num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a, features\n",
    "    )\n",
    "    nsloss = NSLoss(num_nodes, num_sampled, embedding_size)\n",
    "\n",
    "    model.to(device)\n",
    "    nsloss.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{\"params\": model.parameters()}, {\"params\": nsloss.parameters()}], lr=1e-4\n",
    "    )\n",
    "\n",
    "    print(f'Beginning training for {feature_name} model')\n",
    "\n",
    "    best_score = 0\n",
    "    test_score = (0.0, 0.0, 0.0)\n",
    "    patience = 0\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_pairs)\n",
    "        batches = get_batches(train_pairs, neighbors, batch_size)\n",
    "\n",
    "        data_iter = tqdm(\n",
    "            batches,\n",
    "            desc=\"epoch %d\" % (epoch),\n",
    "            total=(len(train_pairs) + (batch_size - 1)) // batch_size,\n",
    "            bar_format=\"{l_bar}{r_bar}\",\n",
    "        )\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            embs = model(data[0].to(device), data[2].to(device), data[3].to(device),)\n",
    "            loss = nsloss(data[0].to(device), embs, data[1].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if i % 5000 == 0:\n",
    "                post_fix = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"iter\": i,\n",
    "                    \"avg_loss\": avg_loss / (i + 1),\n",
    "                    \"loss\": loss.item(),\n",
    "                }\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        final_model = dict(zip(edge_types, [dict() for _ in range(edge_type_count)]))\n",
    "\n",
    "        #initialize embeddings of all zeros\n",
    "        final_embeddings = np.zeros((num_nodes, embedding_dim * u_num))\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            train_inputs = torch.tensor([i for _ in range(edge_type_count)]).to(device)\n",
    "            train_types = torch.tensor(list(range(edge_type_count))).to(device)\n",
    "            node_neigh = torch.tensor(\n",
    "                [neighbors[i] for _ in range(edge_type_count)]\n",
    "            ).to(device)\n",
    "            node_emb = model(train_inputs, train_types, node_neigh)\n",
    "            for j in range(edge_type_count):\n",
    "                final_model[edge_types[j]][index2word[i]] = (\n",
    "                    node_emb[j].cpu().detach().numpy()\n",
    "                )\n",
    "\n",
    "                #concatenate and save embeddings\n",
    "                #where i loop is over the drugs (572)\n",
    "                #and j loop is over the event_num (65)\n",
    "                #concatenated_embeddings = torch.cat([node_emb[j].cpu().detach() for j in range(u_num)], dim=1)\n",
    "                #final_embeddings[i, :] = concatenated_embeddings\n",
    "\n",
    "        valid_aucs, valid_f1s, valid_prs = [], [], []\n",
    "        test_aucs, test_f1s, test_prs = [], [], []\n",
    "        for i in range(edge_type_count):\n",
    "            if eval_type == \"all\":\n",
    "                tmp_auc, tmp_f1, tmp_pr = evaluate(\n",
    "                    final_model[edge_types[i]],\n",
    "                    valid_true_data_by_edge[edge_types[i]],\n",
    "                    valid_false_data_by_edge[edge_types[i]],\n",
    "                )\n",
    "                valid_aucs.append(tmp_auc)\n",
    "                valid_f1s.append(tmp_f1)\n",
    "                valid_prs.append(tmp_pr)\n",
    "\n",
    "                tmp_auc, tmp_f1, tmp_pr = evaluate(\n",
    "                    final_model[edge_types[i]],\n",
    "                    testing_true_data_by_edge[edge_types[i]],\n",
    "                    testing_false_data_by_edge[edge_types[i]],\n",
    "                )\n",
    "                test_aucs.append(tmp_auc)\n",
    "                test_f1s.append(tmp_f1)\n",
    "                test_prs.append(tmp_pr)\n",
    "        print(\"valid auc:\", np.mean(valid_aucs))\n",
    "        print(\"valid pr:\", np.mean(valid_prs))\n",
    "        print(\"valid f1:\", np.mean(valid_f1s))\n",
    "\n",
    "        average_auc = np.mean(test_aucs)\n",
    "        average_f1 = np.mean(test_f1s)\n",
    "        average_pr = np.mean(test_prs)\n",
    "\n",
    "        cur_score = np.mean(valid_aucs)\n",
    "        if cur_score > best_score:\n",
    "            best_score = cur_score\n",
    "            test_score = (average_auc, average_f1, average_pr)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > patience_num:\n",
    "                print(\"Early Stopping\")\n",
    "                break\n",
    "    return test_score, final_model #final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FEATURE_EMBEDDINGS = {}\n",
    "_GRAPH_TEST_SCORES = {}\n",
    "\n",
    "def construct_local_embeddings_path(list_name: str) -> Path:\n",
    "    return _CHECKMARKS_FOLDER / f\"{list_name}_embeddings.parquet\"\n",
    "\n",
    "def train_gnn_and_generate_embeddings() -> None:\n",
    "    for name, feature_matrix in _FEATURE_JAC_MATRICES.items():\n",
    "        #check if we've already processed this matrix (useful for colab limits)\n",
    "        if name in _FEATURE_EMBEDDINGS:\n",
    "            pass\n",
    "\n",
    "        #load the train and test files using the helper functions\n",
    "        training_data_by_type = load_training_data(file_name + '/train.txt')\n",
    "        valid_true_data_by_edge, valid_false_data_by_edge = load_testing_data(file_name + '/valid.txt')\n",
    "        testing_true_data_by_edge, testing_false_data_by_edge = load_testing_data(file_name + '/test.txt')\n",
    "\n",
    "        #Obtain model outputs and print results\n",
    "        _GRAPH_TEST_SCORES[name], _FEATURE_EMBEDDINGS[name] = train_model(training_data_by_type, feature_matrix, name)\n",
    "        average_auc, average_f1, average_pr = _GRAPH_TEST_SCORES[name]\n",
    "\n",
    "        print(f'Test Scores for {name} model:')\n",
    "        print(f'AUC: {average_auc}')\n",
    "        print(f'f1: {average_f1}')\n",
    "        print(f'pr: {average_pr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precomputed_embeddings() -> None:\n",
    "    for feature in _FEATURE_TYPES:\n",
    "        _FEATURE_EMBEDDINGS[feature] = pd.read_parquet(construct_local_embeddings_path(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _TRAIN_GNN_MODELS_AND_GENERATE_EMBEDDINGS():\n",
    "    train_gnn_and_generate_embeddings()\n",
    "else:\n",
    "    load_precomputed_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#select equal number of negative examples from all_neg_df\n",
    "num_negatives = len(event_dfs['pos'])\n",
    "all_neg_sample = event_dfs['neg'].sample(n=num_negatives, random_state=42)\n",
    "\n",
    "# Add an 'event_num' column with a placeholder value\n",
    "all_neg_sample['event_num'] = -1\n",
    "\n",
    "#Combine positive and negative examples for neural network training\n",
    "nn_data = pd.concat([event_dfs['pos'], all_neg_sample]).reset_index(drop=True)\n",
    "\n",
    "#split data into train, validation, and test sets (70-10-20 split)\n",
    "train_df, test_df = train_test_split(nn_data, test_size=0.3, random_state=2)\n",
    "valid_df, test_df = train_test_split(test_df, test_size=(2/3), random_state=2)  #split last 30% into 10% and 20% valid/test\n",
    "\n",
    "test_df['interaction'] = test_df['event_num'].apply(lambda x: 1 if 0 <= x <= 64 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#Dataset for training cases\n",
    "class DrugInteractionDataset(Dataset):\n",
    "    def __init__(self, df, matrix):\n",
    "        self.matrix = torch.tensor(matrix.values, dtype=torch.float32)\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        drug1_idx, drug2_idx, event_number = int(row['drug1']), int(row['drug2']), int(row['event_num'])\n",
    "\n",
    "        #Create feature vector from the embeddings\n",
    "        feature_vector = self.matrix[drug1_idx] * self.matrix[drug2_idx]\n",
    "\n",
    "        #Create label tensor with a 1 at the index of event_num+1\n",
    "        #Where a label at 0 indicates no interaction\n",
    "        label = torch.zeros((event_num+1,), dtype=torch.float32)\n",
    "        label[event_number+1] = 1.0\n",
    "\n",
    "        return feature_vector, label\n",
    "\n",
    "#Dataset for test cases\n",
    "class TestInteractionDataset(Dataset):\n",
    "    def __init__(self, df, enzyme_matrix, target_matrix, substructure_matrix, pathway_matrix):\n",
    "        self.df = df\n",
    "        self.enzyme_matrix = torch.tensor(enzyme_matrix.values, dtype=torch.float32)\n",
    "        self.target_matrix = torch.tensor(target_matrix.values, dtype=torch.float32)\n",
    "        self.substructure_matrix = torch.tensor(substructure_matrix.values, dtype=torch.float32)\n",
    "        self.pathway_matrix = torch.tensor(pathway_matrix.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        drug1_idx, drug2_idx, event_number, interaction = (\n",
    "            int(row['drug1']),\n",
    "            int(row['drug2']),\n",
    "            int(row['event_num']),\n",
    "            int(row['interaction'])\n",
    "        )\n",
    "\n",
    "        # Create feature vectors from the embeddings\n",
    "        enzyme_feature_vector = self.enzyme_matrix[drug1_idx] * self.enzyme_matrix[drug2_idx]\n",
    "        target_feature_vector = self.target_matrix[drug1_idx] * self.target_matrix[drug2_idx]\n",
    "        substructure_feature_vector = self.substructure_matrix[drug1_idx] * self.substructure_matrix[drug2_idx]\n",
    "        pathway_feature_vector = self.pathway_matrix[drug1_idx] * self.pathway_matrix[drug2_idx]\n",
    "\n",
    "        #Create label tensor as a 66-dimensional zero tensor\n",
    "        #65+1 to include the no reaction case\n",
    "        label = torch.zeros(event_num+1, dtype=torch.float32)\n",
    "        # Set to 1 at the event_num index if interaction is 1\n",
    "        if interaction == 1:\n",
    "            label[event_number+1] = 1.0\n",
    "        else:\n",
    "            label[0] = 1.0\n",
    "\n",
    "        return (enzyme_feature_vector,\n",
    "                target_feature_vector,\n",
    "                substructure_feature_vector,\n",
    "                pathway_feature_vector), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "nn_batch_size = 128\n",
    "\n",
    "test_dataset = TestInteractionDataset(test_df, enzyme_df, target_df, substructure_df, pathway_df)\n",
    "\n",
    "# Data loaders\n",
    "enzyme_train_loader = DataLoader(DrugInteractionDataset(train_df, enzyme_df), batch_size=nn_batch_size, shuffle=True)\n",
    "target_train_loader = DataLoader(DrugInteractionDataset(train_df, target_df), batch_size=nn_batch_size, shuffle=True)\n",
    "substructure_train_loader = DataLoader(DrugInteractionDataset(train_df, substructure_df), batch_size=nn_batch_size, shuffle=True)\n",
    "pathway_train_loader = DataLoader(DrugInteractionDataset(train_df, pathway_df), batch_size=nn_batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=nn_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=cross_fold_value, shuffle=True, random_state=state)\n",
    "\n",
    "for model_name, (model, train_loader, optimizer) in models_loaders_optimizers.items():\n",
    "    print(f\"Training {model_name} with {cross_fold_value}-fold cross-validation\")\n",
    "\n",
    "    overall_best_loss = float('inf')\n",
    "    overall_best_model_path = f'./{model_name}_best_model.pth'\n",
    "\n",
    "    #split training data into k-folds\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kfold.split(np.arange(len(train_loader.dataset))), 1):\n",
    "        print(f\"Fold {fold}/{cross_fold_value}\")\n",
    "\n",
    "        #create subsets for the current fold's training and validation data\n",
    "        train_subset = torch.utils.data.Subset(train_loader.dataset, train_idx)\n",
    "        valid_subset = torch.utils.data.Subset(train_loader.dataset, valid_idx)\n",
    "\n",
    "        #create data loaders for the current fold's training and validation subsets\n",
    "        train_subset_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "        valid_subset_loader = DataLoader(valid_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "        fold_best_loss = float('inf')\n",
    "        loss_increase_count = 0  # Reset the early stopping count for each fold\n",
    "\n",
    "        #main training loop for the current fold\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_subset_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                train_outputs = model(inputs)\n",
    "\n",
    "                #convert one hot encoded label to single event_num\n",
    "                train_label_index = labels.max(dim=1)[1]\n",
    "\n",
    "                loss = criterion(train_outputs, train_label_index)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            #calculate validation loss for the current fold and epoch\n",
    "            validation_loss = 0.0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in valid_subset_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    validation_outputs = model(inputs)\n",
    "\n",
    "                    #convert one hot encoded label to single event_num\n",
    "                    validation_label_index = labels.max(dim=1)[1]\n",
    "\n",
    "                    validation_loss += criterion(validation_outputs, validation_label_index).item()\n",
    "\n",
    "            validation_loss /= len(valid_subset_loader)\n",
    "            print(f'[{model_name}] Fold {fold}/{cross_fold_value}, Epoch {epoch + 1}/{num_epochs}, Validation Loss: {validation_loss}')\n",
    "\n",
    "            #check for improvement and implement early stopping if needed\n",
    "            if validation_loss < fold_best_loss:\n",
    "                fold_best_loss = validation_loss\n",
    "                loss_increase_count = 0  #reset count if there is an improvement\n",
    "                if fold_best_loss < overall_best_loss:\n",
    "                    overall_best_loss = fold_best_loss\n",
    "                    torch.save(model.state_dict(), overall_best_model_path)\n",
    "            else:\n",
    "                loss_increase_count += 1  #increase count if no improvement\n",
    "                if loss_increase_count >= patience:\n",
    "                    print(f'[{model_name}] Fold {fold}, Stopping early at epoch {epoch + 1}')\n",
    "                    break\n",
    "\n",
    "    print(f'Finished Training {model_name} with cross-validation')\n",
    "    print(f'Best model saved to {overall_best_model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations\n",
    "\n",
    "1. Rohani, N., Eslahchi, C. Drug-Drug Interaction Predicting by Neural Network Using Integrated Similarity. Sci Rep 9, 13645 (2019). https://doi.org/10.1038/s41598-019-50121-3\n",
    "\n",
    "2. Lin, Xuan, et al. \"KGNN: Knowledge Graph Neural Network for Drug-Drug Interaction Prediction.\" IJCAI. Vol. 380. 2020.\n",
    "\n",
    "3. Al-Rabeah, M.H., Lakizadeh, A. Prediction of drug-drug interaction events using graph neural networks based feature extraction. Sci Rep 12, 15590 (2022). https://doi.org/10.1038/s41598-022-19999-4\n",
    "\n",
    "4. Zhang, C., Lu, Y. & Zang, T. CNN-DDI: a learning-based method for predicting drugâ€“drug interactions using convolution neural networks. BMC Bioinformatics 23 (Suppl 1), 88 (2022). https://doi.org/10.1186/s12859-022-04612-2\n",
    "\n",
    "5. Yifan Deng, Xinran Xu, Yang Qiu, Jingbo Xia, Wen Zhang, Shichao Liu, A multimodal deep learning framework for predicting drugâ€“drug interaction events, Bioinformatics, Volume 36, Issue 15, August 2020, Pages 4316â€“4322, https://doi.org/10.1093/bioinformatics/btaa501"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
