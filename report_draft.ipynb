{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "When multiple drugs are used in treatment of a patient, there is a possibility drugs interact, potentially causing harmful effects.\n",
    "To reduce drug development time and uncover negative interactions before human trials, clinicians need a way to predict these drug interactions. This problem is combinatorially difficult due to the massive number of drugs, variations in drug features such as molecular structure, and the range of potential interactions. By using neural networks, clinicians could uncover patterns in drug interaction, enabling better understanding of drug functionality on the body. This understanding could not only discover negative drug interactions, but also secondary use-cases of drugs for treatment.\n",
    "\n",
    "\n",
    "Traditional model architectures, such as proposed by $Rohani^{1}$ utilize drug-similarity matrices of features such as molecular substructure to predict interactions. These matrices are sub-selected for the most informative and fused into a singular matrix as input for a neural network. These approaches omit critical information about the kind of interactions between drugs, and simply discover any interaction. For clinicians to properly understand drug functionality and safety of use, they must be able to predict the severity and type of drug interactions.\n",
    "\n",
    "\n",
    "Graph Neural Networks have recently been employed to encode and explore various drug interaction types. By encoding drugs as nodes, and interaction types as edges, models can better understand how drugs interact with one another. However, generating graph representations of drug interactions is challenging, particularly in how to combine various features from various sources and how to generate embeddings. $Lin et al^{2}$, generates a graph structure from various features, but only predicts the presence of an interaction from node locality - ignoring the semantic representation of features.\n",
    "\n",
    "\n",
    "$Al-Rabeah^{3}$ improves upon both the traditional and GNN approaches by utilizing graph networks of various features to generate embeddings for feeding into a neural network for classification. The aim is to integrate both the similarity matrices of previous approaches with GNN embeddings of multiple featuresets, in order to represent a heterogeneous network for predicting the types of drug-drug interactions.\n",
    "\n",
    "\n",
    "The work utilizes four feature matrices (Chemical structure, Target, Enzyme, and Pathway) as attributes for nodes representing drugs via similarity matrices, with edges being the drug-drug interaction. Each graph is then used to generate an embedding matrix with a vector for each drug and interaction type. This is done via a random walk from the drug node utilizing edges of the given interaction type to form a node sequence, which is then used to learn embeddings. These four embedding matrices are sliced and cross-multiplied within each other to represent drug-drug interactions. These vectors are then fed into neural networks for each matrix, using a softmax to generate probability vectors for all event types. These vectors are fed through ReLU activation to produce a final prediction vector.\n",
    "\n",
    "\n",
    "The work tested using all subset of the feature matrices, with the best performance from usage all four matrices gathered. The results, achieving 0.9206, 0.9992, 0.9717, 0.8579, and 0.8259 Accuracy, AUC, AUPR, F1, and recall scores respectively, outperformed many recent DDI works. This showed significant improvement over traditional Neural Network approaches, such as CNN-DDI by $Zhang^{4}$. While other GNN approaches, such as the novel KGNN, achieved better performance in some metrics such as F1 score - their analysis was over individual datasets and neglected several features. From this, the GNN-DDI architecture showed exceptional performance as it was faced with multiple-datasets, indicating strong usage across various features.\n",
    "\n",
    "\n",
    "The work of $Al-Rabeah^{3}$ showed the strength of combining various feature sets for generating neural network inputs. Likewise, the paper exemplified the utility in combining multiple components of past approaches (GNN and fusion of multiple similarity matrix results) to produce a robust architecture performant over  features from multiple datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope of Reproducibility\n",
    "\n",
    "\n",
    "We first re-implemented the entire code repository of the paper using PyTorch. The original code provided was extremely hard to understand and modify, using obscure, lengthy matrix transformations to perform general operations. By re-writing in PyTorch, we found it easier to implement various modifications/ablations within the original codebase.\n",
    "\n",
    "\n",
    "With the Pytorch code, we tested the below hypotheses and ablations:\n",
    "\n",
    "\n",
    "1. Hyperparameter Tuning.\n",
    "   By varying the below hyperparameters, we attempt to achieve a higher performance from the model. The paper performs minimal hyperparameter tuning, so we anticipate minor tuning to significantly improve performance. In addition, the original paper uses the Adam optimizer which is relatively fast to converge on local minima. By optimizing parameters, we could achieve a better minimum for training our models.\n",
    "   - Layer size\n",
    "       - Varying the sizes of intermediary layers can constrain the model's performance to fit to training data. By reducing intermediary layer size, we can have a looser bias to the training set. This could be used to lower dropout rate, enabling faster training without sacrificing model generalization.\n",
    "   - Number of dense layers\n",
    "       - The dataset utilized by the paper is unbalanced leading to underfitting. We anticipate increasing the number of dense layers (2) within the neural networks will result in a more biased model - which could better fit the intricacies of less prevalent drug-drug interactions.\n",
    "   - Dropout rate\n",
    "       - The paper uses a dropout rate of 0.3 to inject random noise and reduce bias. By modifying dropout rate, alongside changes to layer size, we anticipate the same model generalization with improved fitting to training data.\n",
    "2. Removing Dropout, and replacing with optimizer weight decay.\n",
    "   Dropout introduces randomness into the network, while weight decay promotes a balanced model by regulating heavy weights. We anticipate weight decay to more effectively account for relations in similarity matrices, particularly for interactions less present in the training set, leading to a higher CV score.\n",
    "3. Replacing the Adam optimizer with SGD.\n",
    "   The paper did not tune the parameters of the Adam optimizer, which likely led to convergence on a suboptimal minima. SGD is less likely to converge on these minima, and should result in better generalizability than untuned Adam.\n",
    "4. Removal of dropout layers.\n",
    "   The proposed GNN relies on two hidden-layer 0.3 rate dropout layers to prevent overfitting. We will test the removal of each hidden layer, as well as both layers. By removing these layers, we can evaluate how tightly model overfits to the similarity matrices.\n",
    "5. Removal of early stopping\n",
    "   The code utilizes early stopping to prevent model overfitting if performance doesnâ€™t improve in 10 epochs. We will remove this limitation, and attempt to overfit our network while retaining dropout. This will help evaluate how injected noise impacts performance.\n",
    "6. Combining of 4 feedforward networks into 1\n",
    "   The paper uses 4 feedforward networks to evaluate each property type independently and aggregates the results, however this does not capture any interactions between property types. We will test concatenating the feature vectors from each property type and using a single feedforward network with this feature vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "\n",
    "The paper utilizes four 2-D feature matrices of 572 drugs, derived from different databases:\n",
    "* Chemical Structure\n",
    "   - SMILE strings, ASCII representations, of drug molecule structures. Commonly used to encode molecular structure.\n",
    "* Target\n",
    "   - The molecule in the body which drug is intended to impact.\n",
    "* Enzyme\n",
    "   - The enzyme responsible for processing a drug within the body. Enzymes can degrade molecules into derivatives.\n",
    "* Pathway\n",
    "   - The specific pathway through which drugs are absorbed, distributed, metabolized, and excreted from the body.\n",
    "\n",
    "\n",
    "All feature matrices were derived from the DrugBank database, and sourced from $Deng et al^{5}$ as an _eventdb_. The Pathway feature matrix also includes data from the KEGG database. Each column in the matrices represents an individual drug, while the rows are 1-hot encodings of the presence of drug properties. As a result, each matrix varies in the length of the second axis.\n",
    "\n",
    "\n",
    "To achieve a representation of drug interactions, each of these feature matrices are transformed into similarity matrices via Jacquard similarity function. This results in uniform 572x572 similarity feature matrices. These similarity matrices were provided, however we also utilized our own code to derive these matrices. The similarity matrices become the node attributes for our various graphs.\n",
    "\n",
    "\n",
    "In addition, the paper utilizes a 4-D drug interaction matrix, describing 65 types of interactions between the 572 drugs. These 65 types are the types of DDI events extracted by $Deng et al^{5}$. This matrix is used as the nodes and edges of our drug graphs. The matrix axes correspond to two drugs, their mechanism for interaction, and if the interaction is increased or decreased. From this, the paper can derive two submatrices: the presence of drug interactions and interaction types. The distribution of the data is extremely uneven across event types, resulting in underfitting to the less represented event. This fact informed the basis of many of our experiments - as we try to appropriately fit the model for these events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Configuration\n",
    "\n",
    "This paper has several steps:\n",
    "* Download event_db and extract raw drug attributes and interaction data.\n",
    "* Train GNN models and generate graph embeddings\n",
    "* Train NN\n",
    "\n",
    "For ease of running, we will enable feature_flags, selecting to either fully run each step OR download from precomputed sources. Any step can be toggled on/off in the below cell.\n",
    "\n",
    "For the best performance, use precomputed data by setting:\n",
    "```_ALWAYS_GENERATE_DATA = False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook feature flags.\n",
    "\n",
    "# == Data Flags ==\n",
    "\n",
    "# Set the below main feature flags for default settings.\n",
    "# If neither selected, default is to use precomputed values.\n",
    "_ALWAYS_GENERATE_DATA = False\n",
    "_ALWAYS_SAVE_DATA = True\n",
    "\n",
    "\n",
    "# Default Feature Flags\n",
    "\n",
    "# == Data Flags == \n",
    "\n",
    "_DOWNLOAD_EVENTDB_AND_CONSTRUCT_JACCARD_MATRICES = False\n",
    "# Download event db and manually construct jaccard similarity matrices\n",
    "\n",
    "_DOWNLOAD_EVENTDB_AND_CONSTRUCT_INTERACTION_MATRICES = False\n",
    "# Download event db and manually construct drug interaction matrices\n",
    "\n",
    "# == Training Flags == \n",
    "\n",
    "_TRAIN_GNN_MODELS_AND_GENERATE_EMBEDDINGS = False\n",
    "\n",
    "_TRAIN_NN = False\n",
    "\n",
    "if _ALWAYS_GENERATE_DATA:\n",
    "    _DOWNLOAD_EVENTDB_AND_CONSTRUCT_JACCARD_MATRICES = True\n",
    "    _DOWNLOAD_EVENTDB_AND_CONSTRUCT_INTERACTION_MATRICES = True\n",
    "    _TRAIN_GNN_MODELS_AND_GENERATE_EMBEDDINGS = True\n",
    "    _TRAIN_NN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tempfile import NamedTemporaryFile\n",
    "import torch\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def download_file(url: str, file_path: Path) -> None:\n",
    "    response = requests.get(url)\n",
    "    with file_path.open('wb') as file:\n",
    "        file.write(response.content)\n",
    "    logging.info(f'Downloaded file to {file_path}')\n",
    "\n",
    "@dataclass\n",
    "class LocalRemotePath:\n",
    "    \"\"\"Dataclass for maintaining a local and remove version of checkpointed data.\"\"\"\n",
    "\n",
    "    local: Path\n",
    "    remote: str\n",
    "\n",
    "    def __init__(self, file_name, remote_url):\n",
    "        self.local = _CHECKMARKS_FOLDER / f\"{file_name}.parquet\"\n",
    "        self.remote = remote_url\n",
    "\n",
    "    def download_as_df(self) -> pd.DataFrame:\n",
    "        if self.local.exists():\n",
    "            logging.info(f\"using local dataframe {self.local}\")\n",
    "            return pd.read_parquet(self.local)\n",
    "        logging.info(f\"Downloading from {self.remote}\")\n",
    "        return pd.read_csv(self.remote, header=None)\n",
    "\n",
    "    def to_parquet(self, dataframe) -> None:\n",
    "        dataframe.to_parquet(self.local)\n",
    "        logging.info(f\"Saved dataframe to {self.local}\")\n",
    "\n",
    "    def save_model(self, state_dict) -> None:\n",
    "        torch.save(state_dict(), self.local)\n",
    "        logging.info(f\"Saved dataframe to {self.local}\")\n",
    "\n",
    "    def load_model_state_dict(self):\n",
    "        if self.local.exists():\n",
    "            return torch.load(self.local)\n",
    "        else:\n",
    "            with NamedTemporaryFile() as f:\n",
    "                print(f.name)\n",
    "                download_file(self.remote, Path(f.name))\n",
    "                return torch.load(f.name, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "\n",
    "_CHECKMARKS_FOLDER = Path.cwd() / \"checkmarks\"\n",
    "_CHECKMARKS_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "_REPO_BASE = \"https://raw.githubusercontent.com/NealRyan/Drug-drug-interaction-modeling/main\"\n",
    "PATH_MAPPING = {\n",
    "    \"jaccard_target\": LocalRemotePath(\"jaccard_target\", f\"{_REPO_BASE}/Data/Similarity%20Matrices/target.txt\"),\n",
    "    \"jaccard_enzyme\": LocalRemotePath(\"jaccard_target\", f\"{_REPO_BASE}/Data/Similarity%20Matrices/enzyme.txt\"),\n",
    "    \"jaccard_pathway\": LocalRemotePath(\"jaccard_pathway\", f\"{_REPO_BASE}/Data/Similarity%20Matrices/pathway.txt\"),\n",
    "    \"jaccard_smile\": LocalRemotePath(\"jaccard_smile\", f\"{_REPO_BASE}/Data/Similarity%20Matrices/substructure.txt\"),\n",
    "    \"pca_jaccard_target\": LocalRemotePath(\"pca_jaccard_target\", f\"{_REPO_BASE}/Original%20Code/DDI/target_PCA.csv\"),\n",
    "    \"pca_jaccard_enzyme\": LocalRemotePath(\"pca_jaccard_target\", f\"{_REPO_BASE}/Original%20Code/DDI/enzyme_PCA.csv\"),\n",
    "    \"pca_jaccard_pathway\": LocalRemotePath(\"pca_jaccard_pathway\", f\"{_REPO_BASE}/Original%20Code/DDI/pathway_PCA.csv\"),\n",
    "    \"pca_jaccard_smile\": LocalRemotePath(\"pca_jaccard_smile\", f\"{_REPO_BASE}/Original%20Code/DDI/smile_PCA.csv\"),\n",
    "    \"pos\": LocalRemotePath(\"pos\", f\"{_REPO_BASE}/Original%20Code/DDI/full_pos2.txt\"),\n",
    "    \"neg\": LocalRemotePath(\"neg\", f\"{_REPO_BASE}/Original%20Code/DDI/all_neg2.txt\"),\n",
    "    \"raw\": LocalRemotePath(\"raw\", f\"{_REPO_BASE}/Data/Encoded%20Features%20Data/event_number.csv\"),\n",
    "    \"embeddings_enzyme\": LocalRemotePath(\"embeddings_enzyme\", f\"{_REPO_BASE}/Data/Embedding%20Matrices/enzyme_drug_embedding_matrix.csv\"),\n",
    "    \"embeddings_target\": LocalRemotePath(\"embeddings_target\", f\"{_REPO_BASE}/Data/Embedding%20Matrices/target_drug_embedding_matrix.csv\"),\n",
    "    \"embeddings_smile\": LocalRemotePath(\"embeddings_substructure\", f\"{_REPO_BASE}/Data/Embedding%20Matrices/substructure_drug_embedding_matrix.csv\"),\n",
    "    \"embeddings_pathway\": LocalRemotePath(\"embeddings_pathway\", f\"{_REPO_BASE}/Data/Embedding%20Matrices/pathway_drug_embedding_matrix.csv\"),\n",
    "    \"graph_scores_enzyme\": LocalRemotePath(\"graph_scores_enzyme\", f\"{_REPO_BASE}\"),\n",
    "    \"graph_scores_target\": LocalRemotePath(\"graph_scores_target\", f\"{_REPO_BASE}\"),\n",
    "    \"graph_scores_smile\": LocalRemotePath(\"graph_scores_substructure\", f\"{_REPO_BASE}\"),\n",
    "    \"graph_scores_pathway\": LocalRemotePath(\"graph_scores_pathway\", f\"{_REPO_BASE}\"),\n",
    "    \"model_enzyme\": LocalRemotePath(\"model_enzyme\", f\"https://github.com/NealRyan/Drug-drug-interaction-modeling/raw/main/Model%20Files/enzyme_model_best_model.pth\"),\n",
    "    \"model_target\": LocalRemotePath(\"model_target\", f\"https://github.com/NealRyan/Drug-drug-interaction-modeling/raw/main/Model%20Files/target_model_best_model.pth\"),\n",
    "    \"model_smile\": LocalRemotePath(\"model_substructure\", f\"https://github.com/NealRyan/Drug-drug-interaction-modeling/raw/main/Model%20Files/pathway_model_best_model.pth\"),\n",
    "    \"model_pathway\": LocalRemotePath(\"model_pathway\", f\"https://github.com/NealRyan/Drug-drug-interaction-modeling/raw/main/Model%20Files/substructure_model_best_model.pth\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Raw Feature Data\n",
    "\n",
    "Download and extract raw feature data from event_db (from Deng et al) and convert into the required Jacard similarity matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using precomputed similarity matrices.\n",
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/jaccard_target.parquet\n",
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/pca_jaccard_target.parquet\n",
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/jaccard_pathway.parquet\n",
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/pca_jaccard_pathway.parquet\n",
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/jaccard_target.parquet\n",
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/pca_jaccard_target.parquet\n",
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/jaccard_smile.parquet\n",
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/pca_jaccard_smile.parquet\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from typing import List, Mapping\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Constants\n",
    "\n",
    "_EVENT_DB_URL: str = 'https://raw.githubusercontent.com/NealRyan/Drug-drug-interaction-modeling/main/Data/Raw%20Event%20DB/event.db'\n",
    "_EVENT_DB_FILE_PATH: Path = _CHECKMARKS_FOLDER / \"event_db.txt\"\n",
    "\n",
    "_FEATURE_TYPES = {\"target\", \"enzyme\", \"pathway\", \"smile\"}\n",
    "_FEATURE_JAC_MATRICES = {}\n",
    "_FEATURE_PCA_MATRICES = {}\n",
    "\n",
    "def download_and_generate_matrices():\n",
    "    \"\"\"Manually download and generate Jaccard similarity matrices.\"\"\"\n",
    "\n",
    "    # Download event and extract raw features\n",
    "    download_file(_EVENT_DB_URL, _EVENT_DB_FILE_PATH)\n",
    "\n",
    "    conn = sqlite3.connect(_EVENT_DB_FILE_PATH)\n",
    "\n",
    "    df_raw_drug = pd.read_sql('select * from drug;', conn)\n",
    "\n",
    "    # close the DB connection\n",
    "    conn.close()\n",
    "\n",
    "    # Transform downloaded raw features into jaccard similarity matrices.\n",
    "    num_drugs = len(df_raw_drug.index)\n",
    "    for feature in _FEATURE_TYPES:\n",
    "        raw_feature_series = df_raw_drug[feature]\n",
    "\n",
    "        # Generate a global list of all elements\n",
    "        # and a mapping of drug idx to its relevant elements\n",
    "        element_list = []\n",
    "        drug_element_mapping = {}\n",
    "        for drug_idx, cell in enumerate(raw_feature_series):\n",
    "            split_element_list = cell.split('|')\n",
    "            drug_element_mapping[drug_idx] = list(range(len(element_list), len(element_list) + len(split_element_list)))\n",
    "            element_list.extend(split_element_list)\n",
    "\n",
    "        # For each drug, set its relevant elements values to False\n",
    "        jaccard = np.full((num_drugs, len(element_list)), False)\n",
    "        for drug_idx, element_idxs in drug_element_mapping.items():\n",
    "            jaccard[drug_idx, element_idxs] = True\n",
    "\n",
    "        jac_sim = 1 - pairwise_distances(jaccard, metric='jaccard')\n",
    "\n",
    "        jac_sim_df = pd.DataFrame(jac_sim)\n",
    "        \n",
    "        # Perform PCA on jac_sim\n",
    "        pca = PCA(n_components=len(jac_sim))\n",
    "        pca.fit(jac_sim)\n",
    "        jac_pca_df = pd.DataFrame(pca.transform(jac_sim))\n",
    "\n",
    "        # Save dataclass for later use.\n",
    "        _FEATURE_JAC_MATRICES[feature] = jac_sim_df\n",
    "        _FEATURE_PCA_MATRICES[feature] = jac_pca_df\n",
    "\n",
    "        if _ALWAYS_SAVE_DATA:\n",
    "            PATH_MAPPING[f\"jaccard_{feature}\"].to_parquet(jac_sim_df)\n",
    "            PATH_MAPPING[f\"pca_jaccard_{feature}\"].to_parquet(jac_pca_df)\n",
    "            \n",
    "\n",
    "def load_precomputed_similarity_matrices() -> None:\n",
    "    \"\"\"Load precomputed Jacard similarity matrices.\"\"\"\n",
    "    logging.info(\"Using precomputed similarity matrices.\")\n",
    "    for feature in _FEATURE_TYPES:\n",
    "        _FEATURE_JAC_MATRICES[feature] = PATH_MAPPING[f\"jaccard_{feature}\"].download_as_df()\n",
    "        _FEATURE_PCA_MATRICES[feature] = PATH_MAPPING[f\"pca_jaccard_{feature}\"].download_as_df()\n",
    "\n",
    "if _DOWNLOAD_EVENTDB_AND_CONSTRUCT_JACCARD_MATRICES:\n",
    "    download_and_generate_matrices()\n",
    "else:\n",
    "    load_precomputed_similarity_matrices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract drug event type interactions from event_db (from Deng et al)\n",
    "\n",
    "These interactions will be used to form our positive examples, negative examples, and a complete list of drug-drug interactions for use in generating our graph embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/pos.parquet\n",
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/neg.parquet\n",
      "INFO:root:using local dataframe /Users/bjones325/Drug-drug-interaction-modeling/checkmarks/raw.parquet\n"
     ]
    }
   ],
   "source": [
    "event_dfs = {\n",
    "    'pos': None,\n",
    "    'neg': None,\n",
    "    'raw': None,\n",
    "}\n",
    "\n",
    "\n",
    "def generate_drug_interaction_matrices() -> None:\n",
    "    conn = sqlite3.connect(_EVENT_DB_FILE_PATH)\n",
    "    df_raw_extraction = pd.read_sql('select * from extraction;', conn)\n",
    "    df_raw_drug = pd.read_sql('select * from drug;', conn)\n",
    "    conn.close()\n",
    "\n",
    "    df_raw_drug = df_raw_drug.set_index('name') \n",
    "\n",
    "    df_raw_extraction['event'] = df_raw_extraction['mechanism'] + ' ' + df_raw_extraction['action']\n",
    "\n",
    "    df_raw_events = df_raw_extraction['event'].value_counts().to_frame()\n",
    "    df_raw_events['event_index'] = np.arange(df_raw_events.shape[0])\n",
    "\n",
    "    df_raw_full_pos = df_raw_extraction.join(df_raw_events, on='event')[['drugA', 'drugB', 'event_index']]\n",
    "    df_raw_full_pos = df_raw_full_pos.join(df_raw_drug['index'], on='drugA')\n",
    "    df_raw_full_pos = df_raw_full_pos.join(df_raw_drug['index'], on='drugB', rsuffix='_drugB')\n",
    "    df_raw_full_pos = df_raw_full_pos[['event_index', 'index', 'index_drugB']]\n",
    "\n",
    "    df_raw_events = df_raw_events.reset_index()[['event', 'count']]\n",
    "\n",
    "    used_pairs = np.identity(len(df_raw_drug.index))\n",
    "    # could also sort ints to ensure consistent ordering instead of making matrix symmetrical\n",
    "    for _, row in df_raw_full_pos.iterrows():\n",
    "        used_pairs[row['index'], row['index_drugB']] = 1\n",
    "        used_pairs[row['index_drugB'], row['index']] = 1\n",
    "\n",
    "    df_raw_all_neg = np.zeros((len(df_raw_full_pos.index), 2))\n",
    "\n",
    "    count = 0\n",
    "    while count < len(df_raw_all_neg):\n",
    "        # could also sort ints to ensure consistent ordering instead of making matrix symmetrical\n",
    "        rand_pair = np.random.randint(len(df_raw_drug.index), size=2)\n",
    "        while used_pairs[rand_pair[0], rand_pair[1]] == 1 or used_pairs[rand_pair[1], rand_pair[0]] == 1:\n",
    "            rand_pair = np.random.randint(len(df_raw_drug.index), size=2)\n",
    "\n",
    "        used_pairs[rand_pair[0], rand_pair[1]] = 1\n",
    "        used_pairs[rand_pair[1], rand_pair[0]] = 1\n",
    "        df_raw_all_neg[count] = rand_pair\n",
    "        count += 1\n",
    "\n",
    "    df_raw_all_neg = pd.DataFrame(df_raw_all_neg, columns=['drug1', 'drug2'], dtype=np.int64)\n",
    "    df_raw_full_pos = df_raw_full_pos.rename(columns={'event_index': 'event_num', 'index': 'drug1', 'index_drugB': 'drug2'})\n",
    "\n",
    "    event_dfs['pos'] = df_raw_full_pos\n",
    "    event_dfs['neg'] = df_raw_all_neg\n",
    "    event_dfs['raw'] = df_raw_events\n",
    "\n",
    "    if _ALWAYS_SAVE_DATA:\n",
    "        for key, df in event_dfs.items():\n",
    "            PATH_MAPPING[key].to_parquet(df)\n",
    "        \n",
    "\n",
    "def load_precomputed_interaction_matrices() -> None:\n",
    "    for key in event_dfs.keys():\n",
    "        event_dfs[key] = PATH_MAPPING[key].download_as_df()\n",
    "\n",
    "if _DOWNLOAD_EVENTDB_AND_CONSTRUCT_INTERACTION_MATRICES:\n",
    "    generate_drug_interaction_matrices()\n",
    "else:\n",
    "    load_precomputed_interaction_matrices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN Training for Embedding Extraction\n",
    "\n",
    "We will create graph embeddings. This will serve as the raw feature vector into our feed forward neural network.\n",
    "\n",
    "To perform this, you will make a graph of nodes (drugs) and edges (events - event num). Four graphs in total are made (one for each of the properties we want to make embeddings for - enzyme, pathway, substructure, and target). The features of the nodes on the graph will be a column of the feature matrix, corresponding to it's similarity to each of the other drugs in the network on that property (enzyme for instance). A random walk of the graph with a context window is used to create the embeddings.\n",
    "\n",
    "The output dimension of these graph embeddings is 572x32x65 which corresponds to the number of drugs x the embedding dimension x the number of events. The embedding dimension is a tuned hyperparameter from the original work, but can theoretically be changed. Finally, the last two dimensions of these matrices will be concatenated to form raw feature vectors for the nerual network (of dimension 572x2080 where 2080=32x65). \"Raw\" because we will still need to perform some further operations before they are ready to go. This process is represented pictorially below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GNN Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General training embeddings.\n",
    "\n",
    "embedding_dim = 32  #Default 32\n",
    "event_num = 65  #Default 65 DO NOT CHANGE\n",
    "walk_length = 10 #Default 10\n",
    "walks_per_node = 20 #Default 20\n",
    "window_size = 5 #Default 5\n",
    "negative_samples = 5 #Default 5\n",
    "learning_rate = 0.01 #Default 0.01\n",
    "epochs = 1 #Default 1\n",
    "batch_size=64 #Default 64, lower this if RAM spikes are crashing Colab\n",
    "num_workers=16 #Default 16\n",
    "patience_num = 5 #no default (because we default to 1 epoch)\n",
    "save_path = _CHECKMARKS_FOLDER\n",
    "edge_dim = 10 #Degault 10\n",
    "att_dim = 20 #Default 20\n",
    "negative_samples = 5 #Default 5\n",
    "neighbor_samples = 20 #Default 10\n",
    "eval_type = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk Helper Functions and Graph class definition.\n",
    "\n",
    "import random\n",
    "import multiprocess as mp\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def walk(args):\n",
    "    walk_length, start, schema = args\n",
    "    # Simulate a random walk starting from start node.\n",
    "    rand = random.Random()\n",
    "\n",
    "    if schema:\n",
    "        schema_items = schema.split('-')\n",
    "        assert schema_items[0] == schema_items[-1]\n",
    "\n",
    "    walk = [start]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        candidates = []\n",
    "        for node in G[cur]:\n",
    "            if schema == '' or node_type[node] == schema_items[len(walk) % (len(schema_items) - 1)]:\n",
    "                candidates.append(node)\n",
    "        if candidates:\n",
    "            walk.append(rand.choice(candidates))\n",
    "        else:\n",
    "            break\n",
    "    return [str(node) for node in walk]\n",
    "\n",
    "def initializer(init_G, init_node_type):\n",
    "    global G\n",
    "    G = init_G\n",
    "    global node_type\n",
    "    node_type = init_node_type\n",
    "\n",
    "class RWGraph():\n",
    "    def __init__(self, nx_G, node_type_arr=None, num_workers=16):\n",
    "        self.G = nx_G\n",
    "        self.node_type = node_type_arr\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def node_list(self, nodes, num_walks):\n",
    "        for loop in range(num_walks):\n",
    "            for node in nodes:\n",
    "                yield node\n",
    "\n",
    "    def simulate_walks(self, num_walks, walk_length, schema=None):\n",
    "        all_walks = []\n",
    "        nodes = list(self.G.keys())\n",
    "        random.shuffle(nodes)\n",
    "\n",
    "        if schema is None:\n",
    "            with mp.Pool(self.num_workers, initializer=initializer, initargs=(self.G, self.node_type)) as pool:\n",
    "                all_walks = list(pool.imap(walk, ((walk_length, node, '') for node in tqdm(self.node_list(nodes, num_walks))), chunksize=256))\n",
    "        else:\n",
    "            schema_list = schema.split(',')\n",
    "            for schema_iter in schema_list:\n",
    "                with mp.Pool(self.num_workers, initializer=initializer, initargs=(self.G, self.node_type)) as pool:\n",
    "                    walks = list(pool.imap(walk, ((walk_length, node, schema_iter) for node in tqdm(self.node_list(nodes, num_walks)) if schema_iter.split('-')[0] == self.node_type[node]), chunksize=512))\n",
    "                all_walks.extend(walks)\n",
    "\n",
    "        return all_walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define graph edges\n",
    "There are two choices for how to define the graph neural network:\n",
    "* We can either include negative samples (drug-drug pairs that DO NOT interact with eachother)\n",
    "* Omit negative examples.\n",
    "\n",
    "To emulate the original paper, we will use Node2Vec, which does *not* use negative examples. Thus we will only use the information from full_pos_df. If you change to another graph technique, it may be useful to use positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for processing event data into graphs and generating graph walks.\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from six import iteritems\n",
    "from sklearn.metrics import (auc, f1_score, precision_recall_curve,\n",
    "                             roc_auc_score)\n",
    "\n",
    "@dataclass\n",
    "class Vocab():\n",
    "    count: int\n",
    "    index: int\n",
    "\n",
    "#Batches for training the GNN\n",
    "def get_batches(pairs, neighbors, batch_size):\n",
    "    n_batches = (len(pairs) + (batch_size - 1)) // batch_size\n",
    "\n",
    "    for idx in range(n_batches):\n",
    "        x, y, t, neigh = [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            index = idx * batch_size + i\n",
    "            if index >= len(pairs):\n",
    "                break\n",
    "            x.append(pairs[index][0])\n",
    "            y.append(pairs[index][1])\n",
    "            t.append(pairs[index][2])\n",
    "            neigh.append(neighbors[pairs[index][0]])\n",
    "        yield torch.tensor(x), torch.tensor(y), torch.tensor(t), torch.tensor(neigh)\n",
    "\n",
    "def get_G_from_edges(edges):\n",
    "    edge_dict = defaultdict(set)\n",
    "    for edge in edges:\n",
    "        u, v = str(edge[0]), str(edge[1])\n",
    "        edge_dict[u].add(v)\n",
    "        edge_dict[v].add(u)\n",
    "    return edge_dict\n",
    "\n",
    "def load_training_data(dataframe):\n",
    "    edge_data_by_type = defaultdict(list)\n",
    "    all_nodes = list()\n",
    "    for _, row in dataframe.iterrows():\n",
    "        event_num, x, y = row[\"event_num\"], row[\"drug1\"], row[\"drug2\"]\n",
    "        edge_data_by_type[event_num].append((x,y))\n",
    "        all_nodes.append(x)\n",
    "        all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    logging.info('Total training nodes: ' + str(len(all_nodes)))\n",
    "    return edge_data_by_type\n",
    "\n",
    "\n",
    "def load_testing_data(dataframe):\n",
    "    true_edge_data_by_type = defaultdict(list)\n",
    "    false_edge_data_by_type = defaultdict(list)\n",
    "    all_nodes = list()\n",
    "    for _, row in dataframe.iterrows():\n",
    "        event_num, x, y, interaction = row[\"event_num\"], row[\"drug1\"], row[\"drug2\"], row[\"interaction\"]\n",
    "        if interaction == 1:\n",
    "            true_edge_data_by_type[event_num].append((x, y))\n",
    "        else:\n",
    "            false_edge_data_by_type[event_num].append((x, y))\n",
    "        all_nodes.append(x)\n",
    "        all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    return true_edge_data_by_type, false_edge_data_by_type\n",
    "\n",
    "def generate_walks(network_data, num_walks, walk_length, schema, file_name, num_workers):\n",
    "    if schema is not None:\n",
    "        node_type = load_node_type(file_name + '/node_type.txt')\n",
    "    else:\n",
    "        node_type = None\n",
    "\n",
    "    all_walks = []\n",
    "    for layer_id, layer_name in enumerate(network_data):\n",
    "        tmp_data = network_data[layer_name]\n",
    "        # start to do the random walk on a layer\n",
    "\n",
    "        layer_walker = RWGraph(get_G_from_edges(tmp_data), node_type, num_workers)\n",
    "        logging.debug('Generating random walks for event_num', layer_id)\n",
    "        layer_walks = layer_walker.simulate_walks(num_walks, walk_length, schema=schema)\n",
    "\n",
    "        all_walks.append(layer_walks)\n",
    "\n",
    "    logging.info('Finish generating the walks')\n",
    "\n",
    "    return all_walks\n",
    "\n",
    "def generate_pairs(all_walks, vocab, window_size, num_workers):\n",
    "    pairs = []\n",
    "    skip_window = window_size // 2\n",
    "    for layer_id, walks in enumerate(all_walks):\n",
    "        logging.debug('Generating training pairs for event_num', layer_id)\n",
    "        for walk in tqdm(walks):\n",
    "            for i in range(len(walk)):\n",
    "                for j in range(1, skip_window + 1):\n",
    "                    if i - j >= 0:\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i - j]].index, layer_id))\n",
    "                    if i + j < len(walk):\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i + j]].index, layer_id))\n",
    "    return pairs\n",
    "\n",
    "def generate_vocab(all_walks):\n",
    "    index2word = []\n",
    "    raw_vocab = defaultdict(int)\n",
    "\n",
    "    for layer_id, walks in enumerate(all_walks):\n",
    "        logging.info('Counting vocab for event_num', layer_id)\n",
    "        for walk in tqdm(walks):\n",
    "            for word in walk:\n",
    "                raw_vocab[word] += 1\n",
    "\n",
    "    vocab = {}\n",
    "    for word, v in iteritems(raw_vocab):\n",
    "        vocab[word] = Vocab(count=v, index=len(index2word))\n",
    "        index2word.append(word)\n",
    "\n",
    "    index2word.sort(key=lambda word: vocab[word].count, reverse=True)\n",
    "    for i, word in enumerate(index2word):\n",
    "        vocab[word].index = i\n",
    "\n",
    "    return vocab, index2word\n",
    "\n",
    "def load_walks(walk_file):\n",
    "    logging.info('Loading walks')\n",
    "    all_walks = []\n",
    "    with open(walk_file, 'r') as f:\n",
    "        for line in f:\n",
    "            content = line.strip().split()\n",
    "            layer_id = int(content[0])\n",
    "            if layer_id >= len(all_walks):\n",
    "                all_walks.append([])\n",
    "            all_walks[layer_id].append(content[1:])\n",
    "    return all_walks\n",
    "\n",
    "def save_walks(walk_file, all_walks):\n",
    "    with open(walk_file, 'w') as f:\n",
    "        for layer_id, walks in enumerate(all_walks):\n",
    "            logging.debug('Saving walks for event_num', layer_id)\n",
    "            for walk in tqdm(walks):\n",
    "                f.write(' '.join([str(layer_id)] + [str(x) for x in walk]) + '\\n')\n",
    "\n",
    "def generate(network_data, num_walks, walk_length, schema, file_name, window_size, num_workers, walk_file):\n",
    "    if walk_file is not None:\n",
    "        all_walks = load_walks(walk_file)\n",
    "    else:\n",
    "        all_walks = generate_walks(network_data, num_walks, walk_length, schema, file_name, num_workers)\n",
    "        save_walks(file_name / 'walks.txt', all_walks)\n",
    "    vocab, index2word = generate_vocab(all_walks)\n",
    "    train_pairs = generate_pairs(all_walks, vocab, window_size, num_workers)\n",
    "\n",
    "    return vocab, index2word, train_pairs\n",
    "\n",
    "def generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples):\n",
    "    edge_type_count = len(edge_types)\n",
    "    neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]\n",
    "    for r in range(edge_type_count):\n",
    "        logging.debug('Generating neighbors for event_num', r)\n",
    "        g = network_data[edge_types[r]]\n",
    "        for (x, y) in tqdm(g):\n",
    "            ix = vocab[x].index\n",
    "            iy = vocab[y].index\n",
    "            neighbors[ix][r].append(iy)\n",
    "            neighbors[iy][r].append(ix)\n",
    "        for i in range(num_nodes):\n",
    "            if len(neighbors[i][r]) == 0:\n",
    "                neighbors[i][r] = [i] * neighbor_samples\n",
    "            elif len(neighbors[i][r]) < neighbor_samples:\n",
    "                neighbors[i][r].extend(list(np.random.choice(neighbors[i][r], size=neighbor_samples-len(neighbors[i][r]))))\n",
    "            elif len(neighbors[i][r]) > neighbor_samples:\n",
    "                neighbors[i][r] = list(np.random.choice(neighbors[i][r], size=neighbor_samples))\n",
    "    return neighbors\n",
    "\n",
    "def get_score(local_model, node1, node2):\n",
    "    try:\n",
    "        vector1 = local_model[node1]\n",
    "        vector2 = local_model[node2]\n",
    "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "def evaluate(model, true_edges, false_edges):\n",
    "    true_list = list()\n",
    "    prediction_list = list()\n",
    "    true_num = 0\n",
    "    for edge in true_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        if tmp_score is not None:\n",
    "            true_list.append(1)\n",
    "            prediction_list.append(tmp_score)\n",
    "            true_num += 1\n",
    "\n",
    "    for edge in false_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        if tmp_score is not None:\n",
    "            true_list.append(0)\n",
    "            prediction_list.append(tmp_score)\n",
    "\n",
    "    sorted_pred = prediction_list[:]\n",
    "    sorted_pred.sort()\n",
    "    threshold = sorted_pred[-true_num]\n",
    "\n",
    "    y_pred = np.zeros(len(prediction_list), dtype=np.int32)\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] >= threshold:\n",
    "            y_pred[i] = 1\n",
    "\n",
    "    y_true = np.array(true_list)\n",
    "    y_scores = np.array(prediction_list)\n",
    "    ps, rs, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return roc_auc_score(y_true, y_scores), f1_score(y_true, y_pred), auc(rs, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model class.\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class GATNEModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a, features\n",
    "    ):\n",
    "        super(GATNEModel, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_u_size = embedding_u_size\n",
    "        self.edge_type_count = edge_type_count\n",
    "        self.dim_a = dim_a\n",
    "\n",
    "        self.features = None\n",
    "        if features is not None:\n",
    "            self.features = features\n",
    "            feature_dim = self.features.shape[-1]\n",
    "            self.embed_trans = Parameter(torch.FloatTensor(feature_dim, embedding_size))\n",
    "            self.u_embed_trans = Parameter(torch.FloatTensor(edge_type_count, feature_dim, embedding_u_size))\n",
    "        else:\n",
    "            self.node_embeddings = Parameter(torch.FloatTensor(num_nodes, embedding_size))\n",
    "            self.node_type_embeddings = Parameter(\n",
    "                torch.FloatTensor(num_nodes, edge_type_count, embedding_u_size)\n",
    "            )\n",
    "        self.trans_weights = Parameter(\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, embedding_size)\n",
    "        )\n",
    "        self.trans_weights_s1 = Parameter(\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, dim_a)\n",
    "        )\n",
    "        self.trans_weights_s2 = Parameter(torch.FloatTensor(edge_type_count, dim_a, 1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.features is not None:\n",
    "            self.embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "            self.u_embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        else:\n",
    "            self.node_embeddings.data.uniform_(-1.0, 1.0)\n",
    "            self.node_type_embeddings.data.uniform_(-1.0, 1.0)\n",
    "        self.trans_weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s1.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s2.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "\n",
    "    def forward(self, train_inputs, train_types, node_neigh):\n",
    "        if self.features is None:\n",
    "            node_embed = self.node_embeddings[train_inputs]\n",
    "            node_embed_neighbors = self.node_type_embeddings[node_neigh]\n",
    "        else:\n",
    "            node_embed = torch.mm(self.features[train_inputs], self.embed_trans)\n",
    "            node_embed_neighbors = torch.einsum('bijk,akm->bijam', self.features[node_neigh], self.u_embed_trans)\n",
    "        node_embed_tmp = torch.diagonal(node_embed_neighbors, dim1=1, dim2=3).permute(0, 3, 1, 2)\n",
    "        node_type_embed = torch.sum(node_embed_tmp, dim=2)\n",
    "\n",
    "        trans_w = self.trans_weights[train_types]\n",
    "        trans_w_s1 = self.trans_weights_s1[train_types]\n",
    "        trans_w_s2 = self.trans_weights_s2[train_types]\n",
    "\n",
    "        attention = F.softmax(\n",
    "            torch.matmul(\n",
    "                torch.tanh(torch.matmul(node_type_embed, trans_w_s1)), trans_w_s2\n",
    "            ).squeeze(2),\n",
    "            dim=1,\n",
    "        ).unsqueeze(1)\n",
    "        node_type_embed = torch.matmul(attention, node_type_embed)\n",
    "        node_embed = node_embed + torch.matmul(node_type_embed, trans_w).squeeze(1)\n",
    "\n",
    "        last_node_embed = F.normalize(node_embed, dim=1)\n",
    "\n",
    "        return last_node_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our custom loss function.\n",
    "\n",
    "class NSLoss(nn.Module):\n",
    "    def __init__(self, num_nodes, num_sampled, embedding_size):\n",
    "        super(NSLoss, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_sampled = num_sampled\n",
    "        self.embedding_size = embedding_size\n",
    "        self.weights = Parameter(torch.FloatTensor(num_nodes, embedding_size))\n",
    "        self.sample_weights = F.normalize(\n",
    "            torch.Tensor(\n",
    "                [\n",
    "                    (math.log(k + 2) - math.log(k + 1)) / math.log(num_nodes + 1)\n",
    "                    for k in range(num_nodes)\n",
    "                ]\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "\n",
    "    def forward(self, input, embs, label):\n",
    "        n = input.shape[0]\n",
    "        log_target = torch.log(\n",
    "            torch.sigmoid(torch.sum(torch.mul(embs, self.weights[label]), 1))\n",
    "        )\n",
    "        negs = torch.multinomial(\n",
    "            self.sample_weights, self.num_sampled * n, replacement=True\n",
    "        ).view(n, self.num_sampled)\n",
    "        noise = torch.neg(self.weights[negs])\n",
    "        sum_log_sampled = torch.sum(\n",
    "            torch.log(torch.sigmoid(torch.bmm(noise, embs.unsqueeze(2)))), 1\n",
    "        ).squeeze()\n",
    "\n",
    "        loss = log_target + sum_log_sampled\n",
    "        return -loss.sum() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our mmodel trainning function.\n",
    "\n",
    "def train_model(network_data, feature_dic, feature_name, valid_true_data_by_edge,\n",
    "            valid_false_data_by_edge,\n",
    "            testing_true_data_by_edge,\n",
    "            testing_false_data_by_edge):\n",
    "    vocab, index2word, train_pairs = generate(network_data, walks_per_node, walk_length, schema=None, file_name=save_path, window_size=window_size, num_workers=num_workers, walk_file=None)\n",
    "\n",
    "    edge_types = list(network_data.keys())\n",
    "\n",
    "    num_nodes = len(index2word)\n",
    "    edge_type_count = len(edge_types)\n",
    "    #epochs = epochs\n",
    "    #batch_size = args.batch_size\n",
    "    embedding_size = embedding_dim\n",
    "    embedding_u_size = edge_dim\n",
    "    u_num = edge_type_count\n",
    "    num_sampled = negative_samples\n",
    "    dim_a = att_dim\n",
    "    att_head = 1\n",
    "    #neighbor_samples = neighbor_samples\n",
    "    logging.debug(u_num)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    neighbors = generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples)\n",
    "\n",
    "    features = None\n",
    "    if feature_dic is not None:\n",
    "        feature_dim = len(list(feature_dic.values())[0])\n",
    "        logging.debug('feature dimension: ' + str(feature_dim))\n",
    "        features = np.zeros((num_nodes, feature_dim), dtype=np.float32)\n",
    "        for key, value in feature_dic.items():\n",
    "            if key in vocab:\n",
    "                features[vocab[key].index, :] = np.array(value)\n",
    "        features = torch.FloatTensor(features).to(device)\n",
    "\n",
    "    model = GATNEModel(\n",
    "        num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a, features\n",
    "    )\n",
    "    nsloss = NSLoss(num_nodes, num_sampled, embedding_size)\n",
    "\n",
    "    model.to(device)\n",
    "    nsloss.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{\"params\": model.parameters()}, {\"params\": nsloss.parameters()}], lr=1e-4\n",
    "    )\n",
    "\n",
    "    logging.info(f'Beginning training for {feature_name} model')\n",
    "\n",
    "    best_score = 0\n",
    "    test_score = (0.0, 0.0, 0.0)\n",
    "    patience = 0\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_pairs)\n",
    "        batches = get_batches(train_pairs, neighbors, batch_size)\n",
    "\n",
    "        data_iter = tqdm(\n",
    "            batches,\n",
    "            desc=\"epoch %d\" % (epoch),\n",
    "            total=(len(train_pairs) + (batch_size - 1)) // batch_size,\n",
    "            bar_format=\"{l_bar}{r_bar}\",\n",
    "        )\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            embs = model(data[0].to(device), data[2].to(device), data[3].to(device),)\n",
    "            loss = nsloss(data[0].to(device), embs, data[1].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if i % 5000 == 0:\n",
    "                post_fix = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"iter\": i,\n",
    "                    \"avg_loss\": avg_loss / (i + 1),\n",
    "                    \"loss\": loss.item(),\n",
    "                }\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        final_model = dict(zip(edge_types, [dict() for _ in range(edge_type_count)]))\n",
    "\n",
    "        #initialize embeddings of all zeros\n",
    "        final_embeddings = np.zeros((num_nodes, embedding_dim * u_num))\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            train_inputs = torch.tensor([i for _ in range(edge_type_count)]).to(device)\n",
    "            train_types = torch.tensor(list(range(edge_type_count))).to(device)\n",
    "            node_neigh = torch.tensor(\n",
    "                [neighbors[i] for _ in range(edge_type_count)]\n",
    "            ).to(device)\n",
    "            node_emb = model(train_inputs, train_types, node_neigh)\n",
    "            for j in range(edge_type_count):\n",
    "                final_model[edge_types[j]][index2word[i]] = (\n",
    "                    node_emb[j].cpu().detach().numpy()\n",
    "                )\n",
    "\n",
    "                #concatenate and save embeddings\n",
    "                #where i loop is over the drugs (572)\n",
    "                #and j loop is over the event_num (65)\n",
    "                #concatenated_embeddings = torch.cat([node_emb[j].cpu().detach() for j in range(u_num)], dim=1)\n",
    "                #final_embeddings[i, :] = concatenated_embeddings\n",
    "\n",
    "        valid_aucs, valid_f1s, valid_prs = [], [], []\n",
    "        test_aucs, test_f1s, test_prs = [], [], []\n",
    "        for i in range(edge_type_count):\n",
    "            if eval_type == \"all\":\n",
    "                tmp_auc, tmp_f1, tmp_pr = evaluate(\n",
    "                    final_model[edge_types[i]],\n",
    "                    valid_true_data_by_edge[edge_types[i]],\n",
    "                    valid_false_data_by_edge[edge_types[i]],\n",
    "                )\n",
    "                valid_aucs.append(tmp_auc)\n",
    "                valid_f1s.append(tmp_f1)\n",
    "                valid_prs.append(tmp_pr)\n",
    "\n",
    "                tmp_auc, tmp_f1, tmp_pr = evaluate(\n",
    "                    final_model[edge_types[i]],\n",
    "                    testing_true_data_by_edge[edge_types[i]],\n",
    "                    testing_false_data_by_edge[edge_types[i]],\n",
    "                )\n",
    "                test_aucs.append(tmp_auc)\n",
    "                test_f1s.append(tmp_f1)\n",
    "                test_prs.append(tmp_pr)\n",
    "        logging.info(\"valid auc:\", np.mean(valid_aucs))\n",
    "        logging.info(\"valid pr:\", np.mean(valid_prs))\n",
    "        logging.info(\"valid f1:\", np.mean(valid_f1s))\n",
    "\n",
    "        average_auc = np.mean(test_aucs)\n",
    "        average_f1 = np.mean(test_f1s)\n",
    "        average_pr = np.mean(test_prs)\n",
    "\n",
    "        cur_score = np.mean(valid_aucs)\n",
    "        if cur_score > best_score:\n",
    "            best_score = cur_score\n",
    "            test_score = (average_auc, average_f1, average_pr)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > patience_num:\n",
    "                logging.info(\"Early Stopping\")\n",
    "                break\n",
    "    return test_score, final_model #final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Train/Test/Validate Splits\n",
    "\n",
    "We will now create our train, test, and validate splits from the drug-drug interaction dataframes we generated from the event_db.\n",
    "\n",
    "We will use a standard (70-10-20, train, validation, test) split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test, train, and validation splits.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#select equal number of negative examples from all_neg_df\n",
    "num_negatives = len(event_dfs['pos'])\n",
    "all_neg_sample = event_dfs['neg'].sample(n=num_negatives, random_state=42)\n",
    "\n",
    "# Add an 'event_num' column with a placeholder value\n",
    "all_neg_sample['event_num'] = -1\n",
    "\n",
    "#Combine positive and negative examples for neural network training\n",
    "nn_data = pd.concat([event_dfs['pos'], all_neg_sample]).reset_index(drop=True)\n",
    "\n",
    "#split data into train, validation, and test sets (70-10-20 split)\n",
    "train_df, test_df = train_test_split(nn_data, test_size=0.3, random_state=2)\n",
    "valid_df, test_df = train_test_split(test_df, test_size=(2/3), random_state=2)  #split last 30% into 10% and 20% valid/test\n",
    "\n",
    "# Define binary value based on presence of an interaction (event_num != -1)\n",
    "test_df['interaction'] = test_df['event_num'].apply(lambda x: 1 if 0 <= x <= 64 else 0)\n",
    "valid_df['interaction'] = valid_df['event_num'].apply(lambda x: 1 if 0 <= x <= 64 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainn our GNNs and generate embeddings for all 4 feature matrices.\n",
    "\n",
    "_FEATURE_EMBEDDINGS = {}\n",
    "_GRAPH_TEST_SCORES = {}\n",
    "\n",
    "def train_gnn_and_generate_embeddings() -> None:\n",
    "    for name, feature_matrix in _FEATURE_JAC_MATRICES.items():\n",
    "        #check if we've already processed this matrix (useful for colab limits)\n",
    "        if name in _FEATURE_EMBEDDINGS:\n",
    "            pass\n",
    "\n",
    "        #load the train and test files using the helper functions\n",
    "        training_data_by_type = load_training_data(train_df)\n",
    "        valid_true_data_by_edge, valid_false_data_by_edge = load_testing_data(valid_df)\n",
    "        testing_true_data_by_edge, testing_false_data_by_edge = load_testing_data(test_df)\n",
    "\n",
    "        #Obtain model outputs and print results\n",
    "        _GRAPH_TEST_SCORES[name], _FEATURE_EMBEDDINGS[name] = train_model(\n",
    "            training_data_by_type,\n",
    "            feature_matrix,\n",
    "            name,\n",
    "            valid_true_data_by_edge,\n",
    "            valid_false_data_by_edge,\n",
    "            testing_true_data_by_edge,\n",
    "            testing_false_data_by_edge)\n",
    "        average_auc, average_f1, average_pr = _GRAPH_TEST_SCORES[name]\n",
    "\n",
    "        if _ALWAYS_SAVE_DATA:\n",
    "            PATH_MAPPING[f\"embeddings_{name}\"].to_parquet(_FEATURE_EMBEDDINGS[name])\n",
    "            PATH_MAPPING[f\"graph_scores_{name}\"].to_parquet(_GRAPH_TEST_SCORES[name])\n",
    "\n",
    "        logging.info(f'Test Scores for {name} model:')\n",
    "        logging.info(f'AUC: {average_auc}')\n",
    "        logging.info(f'f1: {average_f1}')\n",
    "        logging.info(f'pr: {average_pr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precomputed_embeddings() -> None:\n",
    "    for feature in _FEATURE_TYPES:\n",
    "        _FEATURE_EMBEDDINGS[feature] = PATH_MAPPING[f\"embeddings_{feature}\"].download_as_df()\n",
    "        #s_GRAPH_TEST_SCORES[feature] = PATH_MAPPING[f\"graph_scores_{feature}\"].download_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading from https://raw.githubusercontent.com/NealRyan/Drug-drug-interaction-modeling/main/Data/Embedding%20Matrices/enzyme_drug_embedding_matrix.csv\n",
      "INFO:root:Downloading from https://raw.githubusercontent.com/NealRyan/Drug-drug-interaction-modeling/main/Data/Embedding%20Matrices/pathway_drug_embedding_matrix.csv\n",
      "INFO:root:Downloading from https://raw.githubusercontent.com/NealRyan/Drug-drug-interaction-modeling/main/Data/Embedding%20Matrices/target_drug_embedding_matrix.csv\n",
      "INFO:root:Downloading from https://raw.githubusercontent.com/NealRyan/Drug-drug-interaction-modeling/main/Data/Embedding%20Matrices/substructure_drug_embedding_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "if _TRAIN_GNN_MODELS_AND_GENERATE_EMBEDDINGS:\n",
    "    train_gnn_and_generate_embeddings()\n",
    "else:\n",
    "    load_precomputed_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "#### Prepare for Neural Network\n",
    "\n",
    "We currently have 572x2080 drug embedding matrices for each of the four properties we are using to help us predict if two drugs will interact (enzyme, pathway, substructure, and target). For each embedding matrix, we need to take a slice horizontally to represent the drug with all it's graph embedded event information (dimension 1x2080). This vector contains all the graph's embedded knowledge for ONE PARTICULAR drug. As this is a drug-drug interaction task, that won't quite be enough, so we now need to multiply it with each other drug to create the feature vectors for our neural network. We do this by elementwise multiplication of the vectors such that the end result will still be 1x2080.\n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "Replicating the paper, we will utilize a network with the architecture:\n",
    "\n",
    "| Layer               | Input Dim | Output Dim | Activation Function | Dropout Rate |\n",
    "| --------            | --------- | ---------- | ------------------- | ------------ |\n",
    "| Linear              | 2080      | 512        | ReLU                |              |\n",
    "| Batch Normalization | 512       |            |                     |              |\n",
    "| Dropout             |           |            |                     | 0.3          |\n",
    "| Linear              | 512       | 256        | ReLU                |              |\n",
    "| Batch Normalization | 256       |            |                     |              |\n",
    "| Dropout             |           |            |                     | 0.3          |\n",
    "| Linear              | 256       | 65         |                     |              |\n",
    "\n",
    "\n",
    "Separate networks will be instantiated for each of the four features. We then take the average of the four vectors to obtain the final output into a binary vector of dimensions 1x65. This represents a 0-1 vector over the 65 possible interaction events, where a value close to 1 indicates a strong predicted interaction, and a value close to 0 predicts no interaction for those two drugs.\n",
    "\n",
    "#### Training Objectives\n",
    "\n",
    "We will be using an Adam optimizer, due to it's robustness against untuned hyperparameters (as the paper doesn't tune parameters extensively). We will use a learning rate of *0.001*, optimizing CrossEntropyLoss as this is a multi-class problem.\n",
    "\n",
    "We will be using 5-cross-fold validation.\n",
    "\n",
    "#### Model Pretraining\n",
    "\n",
    "Since we are training four separate models, we have pretrained these models and stored in our Github. Based on the notebook configuration (at the top of this notebook), models may be pulled from Github or trainned locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#Dataset for training cases\n",
    "class DrugInteractionDataset(Dataset):\n",
    "    def __init__(self, df, matrix):\n",
    "        self.matrix = torch.tensor(matrix.values, dtype=torch.float32)\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        drug1_idx, drug2_idx, event_number = int(row['drug1']), int(row['drug2']), int(row['event_num'])\n",
    "\n",
    "        #Create feature vector from the embeddings\n",
    "        feature_vector = self.matrix[drug1_idx] * self.matrix[drug2_idx]\n",
    "\n",
    "        #Create label tensor with a 1 at the index of event_num+1\n",
    "        #Where a label at 0 indicates no interaction\n",
    "        label = torch.zeros((event_num+1,), dtype=torch.float32)\n",
    "        label[event_number+1] = 1.0\n",
    "\n",
    "        return feature_vector, label\n",
    "\n",
    "#Dataset for test cases\n",
    "class TestInteractionDataset(Dataset):\n",
    "    def __init__(self, df, enzyme_matrix, target_matrix, substructure_matrix, pathway_matrix):\n",
    "        self.df = df\n",
    "        self.enzyme_matrix = torch.tensor(enzyme_matrix.values, dtype=torch.float32)\n",
    "        self.target_matrix = torch.tensor(target_matrix.values, dtype=torch.float32)\n",
    "        self.substructure_matrix = torch.tensor(substructure_matrix.values, dtype=torch.float32)\n",
    "        self.pathway_matrix = torch.tensor(pathway_matrix.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        drug1_idx, drug2_idx, event_number, interaction = (\n",
    "            int(row['drug1']),\n",
    "            int(row['drug2']),\n",
    "            int(row['event_num']),\n",
    "            int(row['interaction'])\n",
    "        )\n",
    "\n",
    "        # Create feature vectors from the embeddings\n",
    "        enzyme_feature_vector = self.enzyme_matrix[drug1_idx] * self.enzyme_matrix[drug2_idx]\n",
    "        target_feature_vector = self.target_matrix[drug1_idx] * self.target_matrix[drug2_idx]\n",
    "        substructure_feature_vector = self.substructure_matrix[drug1_idx] * self.substructure_matrix[drug2_idx]\n",
    "        pathway_feature_vector = self.pathway_matrix[drug1_idx] * self.pathway_matrix[drug2_idx]\n",
    "\n",
    "        #Create label tensor as a 66-dimensional zero tensor\n",
    "        #65+1 to include the no reaction case\n",
    "        label = torch.zeros(event_num+1, dtype=torch.float32)\n",
    "        # Set to 1 at the event_num index if interaction is 1\n",
    "        if interaction == 1:\n",
    "            label[event_number+1] = 1.0\n",
    "        else:\n",
    "            label[0] = 1.0\n",
    "\n",
    "        return (enzyme_feature_vector,\n",
    "                target_feature_vector,\n",
    "                substructure_feature_vector,\n",
    "                pathway_feature_vector), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Neural Network\n",
    "Data loaders: Prepare data for output prediction\n",
    "The neural network outputs a 1x65 dimensional output vectors that represent interaction or not over 65 event types. In order to prepare labels for training and test data, we will want the labels for the data to also be 1x65 tensors, with a 1 at the value of the event num for the two drugs.\n",
    "\n",
    "##### Training:\n",
    "\n",
    "The logic for training data is to just make a zeros tensor and one-hot encode the tensor at the event_num in the training data\n",
    "\n",
    "##### Testing:\n",
    "\n",
    "The logic for testing data is very similar to training. We start with zeros tensors for each of the four matrices (as the test will combine the output of all four models). If interaction = 1 we then one-hot encode at the index of event num. All four tensors are then passed back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "nn_batch_size = 128\n",
    "\n",
    "test_dataset = TestInteractionDataset(test_df, _FEATURE_EMBEDDINGS[\"enzyme\"], _FEATURE_EMBEDDINGS[\"target\"], _FEATURE_EMBEDDINGS[\"smile\"], _FEATURE_EMBEDDINGS[\"pathway\"])\n",
    "\n",
    "# Data loaders\n",
    "TRAIN_LOADERS = {\n",
    "    feature: DataLoader(DrugInteractionDataset(train_df,  _FEATURE_EMBEDDINGS[feature]), batch_size=nn_batch_size, shuffle=True)\n",
    "    for feature in _FEATURE_TYPES\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=nn_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Parameters\n",
    "\n",
    "layer_1_dim = 512  #Default 512\n",
    "layer_2_dim = 256 #Default 256\n",
    "input_dim = embedding_dim * event_num # DO NOT CHANGE\n",
    "output_dim = event_num+1 # DO NOT CHANGE\n",
    "dropout = 0.3 #Default 0.3\n",
    "learning_rate = 0.001 #Default 0.001 in Keras (what the paper uses)\n",
    "num_epochs = 100 #Default 100\n",
    "patience = 10 #Default 10\n",
    "cross_fold_value = 5 #Default 5\n",
    "state = 0 #random state for cross fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network for training on our four feature matrices.\n",
    "\n",
    "class DrugInteractionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DrugInteractionNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, layer_1_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=layer_1_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc2 = nn.Linear(layer_1_dim, layer_2_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=layer_2_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc3 = nn.Linear(layer_2_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        #x = F.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "NN_DEFS = {}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "for feature in _FEATURE_TYPES:\n",
    "    model = DrugInteractionNetwork(input_dim, output_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    NN_DEFS[feature] = (model, TRAIN_LOADERS[feature], optimizer)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/nj/700rnylx1538c3wlmb2xmcxm0000gn/T/tmpjt1wi8s_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloaded file to /var/folders/nj/700rnylx1538c3wlmb2xmcxm0000gn/T/tmpjt1wi8s_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/nj/700rnylx1538c3wlmb2xmcxm0000gn/T/tmpl4_91w5d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloaded file to /var/folders/nj/700rnylx1538c3wlmb2xmcxm0000gn/T/tmpl4_91w5d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/nj/700rnylx1538c3wlmb2xmcxm0000gn/T/tmp2eu7zqkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloaded file to /var/folders/nj/700rnylx1538c3wlmb2xmcxm0000gn/T/tmp2eu7zqkg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/nj/700rnylx1538c3wlmb2xmcxm0000gn/T/tmp2abqyl3x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloaded file to /var/folders/nj/700rnylx1538c3wlmb2xmcxm0000gn/T/tmp2abqyl3x\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_models():\n",
    "    kfold = KFold(n_splits=cross_fold_value, shuffle=True, random_state=state)\n",
    "\n",
    "    for model_name, (model, train_loader, optimizer) in NN_DEFS.items():\n",
    "        logging.info(f\"Training {model_name} with {cross_fold_value}-fold cross-validation\")\n",
    "\n",
    "        overall_best_loss = float('inf')\n",
    "        overall_best_model_path = f'./{model_name}_best_model.pth'\n",
    "\n",
    "        #split training data into k-folds\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kfold.split(np.arange(len(train_loader.dataset))), 1):\n",
    "            logging.debug(f\"Fold {fold}/{cross_fold_value}\")\n",
    "\n",
    "            #create subsets for the current fold's training and validation data\n",
    "            train_subset = torch.utils.data.Subset(train_loader.dataset, train_idx)\n",
    "            valid_subset = torch.utils.data.Subset(train_loader.dataset, valid_idx)\n",
    "\n",
    "            #create data loaders for the current fold's training and validation subsets\n",
    "            train_subset_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "            valid_subset_loader = DataLoader(valid_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "            fold_best_loss = float('inf')\n",
    "            loss_increase_count = 0  # Reset the early stopping count for each fold\n",
    "\n",
    "            #main training loop for the current fold\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                for inputs, labels in train_subset_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    train_outputs = model(inputs)\n",
    "\n",
    "                    #convert one hot encoded label to single event_num\n",
    "                    train_label_index = labels.max(dim=1)[1]\n",
    "\n",
    "                    loss = criterion(train_outputs, train_label_index)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                #calculate validation loss for the current fold and epoch\n",
    "                validation_loss = 0.0\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in valid_subset_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        validation_outputs = model(inputs)\n",
    "\n",
    "                        #convert one hot encoded label to single event_num\n",
    "                        validation_label_index = labels.max(dim=1)[1]\n",
    "\n",
    "                        validation_loss += criterion(validation_outputs, validation_label_index).item()\n",
    "\n",
    "                validation_loss /= len(valid_subset_loader)\n",
    "                logging.debug(f'[{model_name}] Fold {fold}/{cross_fold_value}, Epoch {epoch + 1}/{num_epochs}, Validation Loss: {validation_loss}')\n",
    "\n",
    "                #check for improvement and implement early stopping if needed\n",
    "                if validation_loss < fold_best_loss:\n",
    "                    fold_best_loss = validation_loss\n",
    "                    loss_increase_count = 0  #reset count if there is an improvement\n",
    "                    if fold_best_loss < overall_best_loss:\n",
    "                        overall_best_loss = fold_best_loss\n",
    "                        \n",
    "                        if _ALWAYS_SAVE_DATA:\n",
    "                            PATH_MAPPING[f\"model_{model_name}\"].save_model(model.state_dict())\n",
    "                else:\n",
    "                    loss_increase_count += 1  #increase count if no improvement\n",
    "                    if loss_increase_count >= patience:\n",
    "                        logging.info(f'[{model_name}] Fold {fold}, Stopping early at epoch {epoch + 1}')\n",
    "                        break\n",
    "\n",
    "        logging.info(f'Finished Training {model_name} with cross-validation')\n",
    "        logging.info(f'Best model saved to {overall_best_model_path}')\n",
    "\n",
    "if _TRAIN_NN:\n",
    "    train_models()\n",
    "else:\n",
    "    for model_name, (model, _, _) in NN_DEFS.items():\n",
    "        model.load_state_dict(PATH_MAPPING[f\"model_{model_name}\"].load_model_state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations\n",
    "\n",
    "1. Rohani, N., Eslahchi, C. Drug-Drug Interaction Predicting by Neural Network Using Integrated Similarity. Sci Rep 9, 13645 (2019). https://doi.org/10.1038/s41598-019-50121-3\n",
    "\n",
    "2. Lin, Xuan, et al. \"KGNN: Knowledge Graph Neural Network for Drug-Drug Interaction Prediction.\" IJCAI. Vol. 380. 2020.\n",
    "\n",
    "3. Al-Rabeah, M.H., Lakizadeh, A. Prediction of drug-drug interaction events using graph neural networks based feature extraction. Sci Rep 12, 15590 (2022). https://doi.org/10.1038/s41598-022-19999-4\n",
    "\n",
    "4. Zhang, C., Lu, Y. & Zang, T. CNN-DDI: a learning-based method for predicting drugâ€“drug interactions using convolution neural networks. BMC Bioinformatics 23 (Suppl 1), 88 (2022). https://doi.org/10.1186/s12859-022-04612-2\n",
    "\n",
    "5. Yifan Deng, Xinran Xu, Yang Qiu, Jingbo Xia, Wen Zhang, Shichao Liu, A multimodal deep learning framework for predicting drugâ€“drug interaction events, Bioinformatics, Volume 36, Issue 15, August 2020, Pages 4316â€“4322, https://doi.org/10.1093/bioinformatics/btaa501"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
